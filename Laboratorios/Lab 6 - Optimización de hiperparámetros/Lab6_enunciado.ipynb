{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b5c0d2440b3e4995a794ded565213150",
        "deepnote_cell_type": "markdown",
        "id": "_Mql1uRoI5v5"
      },
      "source": [
        "<h1><center>Laboratorio 6: Optimizaci√≥n de modelos üß™</center></h1>\n",
        "\n",
        "<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos - Oto√±o 2025</strong></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "bfb94b9656f145ad83e81b75d218cb70",
        "deepnote_cell_type": "markdown",
        "id": "FAPGIlEAI5v8"
      },
      "source": [
        "### Cuerpo Docente:\n",
        "\n",
        "- Profesores: Stefano Schiappacasse, Sebasti√°n Tinoco\n",
        "- Auxiliares: Melanie Pe√±a, Valentina Rojas\n",
        "- Ayudantes: Angelo Mu√±oz, Valentina Z√∫√±iga"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b1b537fdd27c43909a49d3476ce64d91",
        "deepnote_cell_type": "markdown",
        "id": "8NozgbkZI5v9"
      },
      "source": [
        "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados\n",
        "\n",
        "- Nombre de alumno 1: Sebasti√°n Acu√±a U.\n",
        "- Nombre de alumno 2: Mart√≠n Guzm√°n S.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8qWRbJkcwP9"
      },
      "source": [
        "### **Link de repositorio de GitHub:** [Repositorio](https://github.com/sebastianacunau/MDS7202-Laboratorios-y-Proyecto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b7dbdd30ab544cb8a8afe00648a586ae",
        "deepnote_cell_type": "markdown",
        "id": "vHU9DI6wI5v9"
      },
      "source": [
        "### Temas a tratar\n",
        "\n",
        "- Predicci√≥n de demanda usando `xgboost`\n",
        "- B√∫squeda del modelo √≥ptimo de clasificaci√≥n usando `optuna`\n",
        "- Uso de pipelines.\n",
        "\n",
        "\n",
        "### Reglas:\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Fecha de entrega: 6 d√≠as de plazo con descuento de 1 punto por d√≠a. Entregas Martes a las 23:59.\n",
        "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria, pero se recomienda fuertemente asistir.\n",
        "- <u>Prohibidas las copias</u>. Cualquier intento de copia ser√° debidamente penalizado con el reglamento de la escuela.\n",
        "- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no est√©n en u-cursos no ser√°n revisados. Recuerden que el repositorio tambi√©n tiene nota.\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
        "- Pueden usar cualquier material del curso que estimen conveniente.\n",
        "\n",
        "El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "f1c73babb7f74af588a4fa6ae14829e0",
        "deepnote_cell_type": "markdown",
        "id": "U_-sNOuOI5v9"
      },
      "source": [
        "# Importamos librerias √∫tiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cell_id": "51afe4d2df42442b9e5402ffece60ead",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 4957,
        "execution_start": 1699544354044,
        "id": "ekHbM85NI5v9",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "#!pip install -qq xgboost optuna\n",
        "from sklearn import set_config\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6hJXpLCSspz"
      },
      "source": [
        "# El emprendimiento de Fiu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "44d227389a734ac59189c5e0005bc68a",
        "deepnote_cell_type": "markdown",
        "id": "b0bDalAOI5v-"
      },
      "source": [
        "Tras liderar de manera exitosa la implementaci√≥n de un proyecto de ciencia de datos para caracterizar los datos generados en Santiago 2023, el misterioso corp√≥reo **Fiu** se anima y decide levantar su propio negocio de consultor√≠a en machine learning. Tras varias e intensas negociaciones, Fiu logra encontrar su *primera chamba*: predecir la demanda (cantidad de venta) de una famosa productora de bebidas de calibre mundial. Al ver el gran potencial y talento que usted ha demostrado en el campo de la ciencia de datos, Fiu lo contrata como data scientist para que forme parte de su nuevo emprendimiento.\n",
        "\n",
        "Para este laboratorio deben trabajar con los datos `sales.csv` subidos a u-cursos, el cual contiene una muestra de ventas de la empresa para diferentes productos en un determinado tiempo.\n",
        "\n",
        "Para comenzar, cargue el dataset se√±alado y visualice a trav√©s de un `.head` los atributos que posee el dataset.\n",
        "\n",
        "<i><p align=\"center\">Fiu siendo felicitado por su excelente desempe√±o en el proyecto de caracterizaci√≥n de datos</p></i>\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media-front.elmostrador.cl/2023/09/A_UNO_1506411_2440e.jpg\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cell_id": "2f9c82d204b14515ad27ae07e0b77702",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 92,
        "execution_start": 1699544359006,
        "id": "QvMPOqHuI5v-",
        "source_hash": null
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>city</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>pop</th>\n",
              "      <th>shop</th>\n",
              "      <th>brand</th>\n",
              "      <th>container</th>\n",
              "      <th>capacity</th>\n",
              "      <th>price</th>\n",
              "      <th>quantity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>31/01/12</td>\n",
              "      <td>Athens</td>\n",
              "      <td>37.97945</td>\n",
              "      <td>23.71622</td>\n",
              "      <td>672130</td>\n",
              "      <td>shop_1</td>\n",
              "      <td>kinder-cola</td>\n",
              "      <td>glass</td>\n",
              "      <td>500ml</td>\n",
              "      <td>0.96</td>\n",
              "      <td>13280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>31/01/12</td>\n",
              "      <td>Athens</td>\n",
              "      <td>37.97945</td>\n",
              "      <td>23.71622</td>\n",
              "      <td>672130</td>\n",
              "      <td>shop_1</td>\n",
              "      <td>kinder-cola</td>\n",
              "      <td>plastic</td>\n",
              "      <td>1.5lt</td>\n",
              "      <td>2.86</td>\n",
              "      <td>6727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>31/01/12</td>\n",
              "      <td>Athens</td>\n",
              "      <td>37.97945</td>\n",
              "      <td>23.71622</td>\n",
              "      <td>672130</td>\n",
              "      <td>shop_1</td>\n",
              "      <td>kinder-cola</td>\n",
              "      <td>can</td>\n",
              "      <td>330ml</td>\n",
              "      <td>0.87</td>\n",
              "      <td>9848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>31/01/12</td>\n",
              "      <td>Athens</td>\n",
              "      <td>37.97945</td>\n",
              "      <td>23.71622</td>\n",
              "      <td>672130</td>\n",
              "      <td>shop_1</td>\n",
              "      <td>adult-cola</td>\n",
              "      <td>glass</td>\n",
              "      <td>500ml</td>\n",
              "      <td>1.00</td>\n",
              "      <td>20050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>31/01/12</td>\n",
              "      <td>Athens</td>\n",
              "      <td>37.97945</td>\n",
              "      <td>23.71622</td>\n",
              "      <td>672130</td>\n",
              "      <td>shop_1</td>\n",
              "      <td>adult-cola</td>\n",
              "      <td>can</td>\n",
              "      <td>330ml</td>\n",
              "      <td>0.39</td>\n",
              "      <td>25696</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id      date    city       lat      long     pop    shop        brand  \\\n",
              "0   0  31/01/12  Athens  37.97945  23.71622  672130  shop_1  kinder-cola   \n",
              "1   1  31/01/12  Athens  37.97945  23.71622  672130  shop_1  kinder-cola   \n",
              "2   2  31/01/12  Athens  37.97945  23.71622  672130  shop_1  kinder-cola   \n",
              "3   3  31/01/12  Athens  37.97945  23.71622  672130  shop_1   adult-cola   \n",
              "4   4  31/01/12  Athens  37.97945  23.71622  672130  shop_1   adult-cola   \n",
              "\n",
              "  container capacity  price  quantity  \n",
              "0     glass    500ml   0.96     13280  \n",
              "1   plastic    1.5lt   2.86      6727  \n",
              "2       can    330ml   0.87      9848  \n",
              "3     glass    500ml   1.00     20050  \n",
              "4       can    330ml   0.39     25696  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "df = pd.read_csv('sales.csv')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b50db6f2cb804932ae3f9e5748a6ea61",
        "deepnote_cell_type": "markdown",
        "id": "pk4ru76pI5v_"
      },
      "source": [
        "## 1 Generando un Baseline (5 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/O-lan6TkadUAAAAC/what-i-wnna-do-after-a-baseline.gif\">\n",
        "</p>\n",
        "\n",
        "Antes de entrenar un algoritmo, usted recuerda los apuntes de su mag√≠ster en ciencia de datos y recuerda que debe seguir una serie de *buenas pr√°cticas* para entrenar correcta y debidamente su modelo. Despu√©s de un par de vueltas, llega a las siguientes tareas:\n",
        "\n",
        "1. Separe los datos en conjuntos de train (70%), validation (20%) y test (10%). Fije una semilla para controlar la aleatoriedad. [0.5 puntos]\n",
        "2. Implemente un `FunctionTransformer` para extraer el d√≠a, mes y a√±o de la variable `date`. Guarde estas variables en el formato categorical de pandas. [1 punto]\n",
        "3. Implemente un `ColumnTransformer` para procesar de manera adecuada los datos num√©ricos y categ√≥ricos. Use `OneHotEncoder` para las variables categ√≥ricas. `Nota:` Utilice el m√©todo `.set_output(transform='pandas')` para obtener un DataFrame como salida del `ColumnTransformer` [1 punto]\n",
        "4. Guarde los pasos anteriores en un `Pipeline`, dejando como √∫ltimo paso el regresor `DummyRegressor` para generar predicciones en base a promedios. [0.5 punto]\n",
        "5. Entrene el pipeline anterior y reporte la m√©trica `mean_absolute_error` sobre los datos de validaci√≥n. ¬øC√≥mo se interpreta esta m√©trica para el contexto del negocio? [0.5 puntos]\n",
        "6. Finalmente, vuelva a entrenar el `Pipeline` pero esta vez usando `XGBRegressor` como modelo **utilizando los par√°metros por default**. ¬øC√≥mo cambia el MAE al implementar este algoritmo? ¬øEs mejor o peor que el `DummyRegressor`? [1 punto]\n",
        "7. Guarde ambos modelos en un archivo .pkl (uno cada uno) [0.5 puntos]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE Dummy Regressor: 13298.4978\n",
            "MAE XGBOOST: 2573.8992\n"
          ]
        }
      ],
      "source": [
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "set_config(transform_output=\"pandas\")\n",
        "\n",
        "#1\n",
        "random_state = 42\n",
        "train_data, temp_data = train_test_split(df, test_size=0.3, random_state=random_state, shuffle=True)\n",
        "validation_data, test_data = train_test_split(temp_data, test_size=1/3, random_state=random_state, shuffle=True)\n",
        "train_data = train_data.dropna()\n",
        "validation_data = validation_data.dropna()\n",
        "\n",
        "#2\n",
        "def extraer_fecha(df):\n",
        "    df['day'] = pd.to_datetime(df['date']).dt.day.astype('category') #se convierte a tipo categoria segun lo pedido\n",
        "    df['month'] = pd.to_datetime(df['date']).dt.month.astype('category') #se convierte a tipo categoria segun lo pedido\n",
        "    df['year'] = pd.to_datetime(df['date']).dt.year.astype('category') #se convierte a tipo categoria segun lo pedido\n",
        "    return df\n",
        "\n",
        "date_transformer = FunctionTransformer(extraer_fecha, validate=False)\n",
        "\n",
        "\n",
        "df_transformed = date_transformer.fit_transform(train_data)\n",
        "\n",
        "#3\n",
        "numerical_columns = df_transformed.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_columns = df_transformed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "numerical_columns.remove('quantity')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_columns),\n",
        "        ('cat',  OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_columns)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "preprocessor.set_output(transform=\"pandas\")\n",
        "\n",
        "\n",
        "\n",
        "#4\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('date_transformer', date_transformer),\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', DummyRegressor(strategy='mean'))\n",
        "])\n",
        "\n",
        "\n",
        "X_train = train_data.drop(columns=['quantity'])\n",
        "y_train = train_data['quantity']\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "#5\n",
        "X_validation = validation_data.drop(columns=['quantity'])\n",
        "y_validation = validation_data['quantity']\n",
        "\n",
        "predictions = pipeline.predict(X_validation)\n",
        "mae = mean_absolute_error(y_validation, predictions)\n",
        "\n",
        "print(f\"MAE Dummy Regressor: {mae:.4f}\")\n",
        "\n",
        "#6\n",
        "pipeline1 = Pipeline(steps=[\n",
        "    ('date_transformer', date_transformer),\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', XGBRegressor(strategy='mean'))\n",
        "])\n",
        "\n",
        "\n",
        "X_train = train_data.drop(columns=['quantity'])\n",
        "y_train = train_data['quantity']\n",
        "\n",
        "pipeline1.fit(X_train, y_train)\n",
        "\n",
        "predictions1 = pipeline1.predict(X_validation)\n",
        "mae1 = mean_absolute_error(y_validation, predictions1)\n",
        "\n",
        "print(f\"MAE XGBOOST: {mae1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "7e17e46063774ec28226fe300d42ffe0",
        "deepnote_cell_type": "markdown",
        "id": "wnyMINdKI5v_"
      },
      "source": [
        "## 2. Forzando relaciones entre par√°metros con XGBoost (10 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://64.media.tumblr.com/14cc45f9610a6ee341a45fd0d68f4dde/20d11b36022bca7b-bf/s640x960/67ab1db12ff73a530f649ac455c000945d99c0d6.gif\">\n",
        "</p>\n",
        "\n",
        "Un colega aficionado a la econom√≠a le *sopla* que la demanda guarda una relaci√≥n inversa con el precio del producto. Motivado para impresionar al querido corp√≥reo, se propone hacer uso de esta informaci√≥n para mejorar su modelo realizando las siguientes tareas:\n",
        "\n",
        "1. Vuelva a entrenar el `Pipeline` con `XGBRegressor`, pero esta vez forzando una relaci√≥n mon√≥tona negativa entre el precio y la cantidad. Para aplicar esta restricci√≥n ap√≥yese en la siguiente <a href = https://xgboost.readthedocs.io/en/stable/tutorials/monotonic.html>documentaci√≥n</a>. [6 puntos]\n",
        "\n",
        ">Hint 1: Para implementar el constraint se le sugiere hacerlo especificando el nombre de la variable. De ser as√≠, probablemente le sea √∫til **mantener el formato de pandas** antes del step de entrenamiento.\n",
        "\n",
        ">Hint 2: Puede obtener el nombre de las columnas en el paso anterior al modelo regresor mediante el m√©todo `.get_feature_names_out()`\n",
        "\n",
        "2. Luego, vuelva a reportar el `MAE` sobre el conjunto de validaci√≥n. [1 puntos]\n",
        "\n",
        "3. ¬øC√≥mo cambia el error al incluir esta relaci√≥n? ¬øTen√≠a raz√≥n su amigo? [2 puntos]\n",
        "\n",
        "4. Guarde su modelo en un archivo .pkl [1 punto]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE XGBoost with Monotonic Constraint: 2708.6716\n"
          ]
        }
      ],
      "source": [
        "# Create the pipeline\n",
        "pipeline_xgb_constrained = Pipeline(steps=[\n",
        "    ('date_transformer', date_transformer),\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', XGBRegressor())\n",
        "])\n",
        "\n",
        "\n",
        "# Fit the preprocessor to get feature names\n",
        "#The change involves applying the fit_transform to the preprocessor directly\n",
        "# to get the feature names instead of the pipeline slice.\n",
        "X_train_transformed = pipeline_xgb_constrained['preprocessor'].fit_transform(pipeline_xgb_constrained['date_transformer'].fit_transform(X_train))\n",
        "feature_names = pipeline_xgb_constrained['preprocessor'].get_feature_names_out()\n",
        "\n",
        "# Find index of 'price' feature\n",
        "price_index = list(feature_names).index(next((name for name in feature_names if 'price' in name), None))\n",
        "\n",
        "# Create monotonic constraints string # This is the updated line\n",
        "monotonic_constraints = \"(\" + \",\".join(['0'] * price_index + ['-1'] + ['0'] * (len(feature_names) - price_index -1)) + \")\"\n",
        "\n",
        "# Set monotonic constraints in the XGBRegressor\n",
        "pipeline_xgb_constrained.named_steps['regressor'].monotone_constraints = monotonic_constraints\n",
        "\n",
        "# Train the pipeline\n",
        "pipeline_xgb_constrained.fit(X_train, y_train)\n",
        "\n",
        "# Transform validation data using the same pipeline steps\n",
        "#validation_data_transformed = pipeline_xgb_constrained[:-1].transform(validation_data) #This line is not used\n",
        "\n",
        "# Predict and calculate MAE\n",
        "predictions_constrained = pipeline_xgb_constrained.predict(validation_data)\n",
        "mae_constrained = mean_absolute_error(validation_data['quantity'], predictions_constrained)\n",
        "print(f\"MAE XGBoost with Monotonic Constraint: {mae_constrained:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e59ef80ed20b4de8921f24da74e87374",
        "deepnote_cell_type": "markdown",
        "id": "5D5-tX4dI5v_"
      },
      "source": [
        "## 1.3 Optimizaci√≥n de Hiperpar√°metros con Optuna (20 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/fmNdyGN4z5kAAAAi/hacking-lucy.gif\">\n",
        "</p>\n",
        "\n",
        "Luego de presentarle sus resultados, Fiu le pregunta si es posible mejorar *aun m√°s* su modelo. En particular, le comenta de la optimizaci√≥n de hiperpar√°metros con metodolog√≠as bayesianas a trav√©s del paquete `optuna`. Como usted es un aficionado al entrenamiento de modelos de ML, se propone implementar la descabellada idea de su jefe.\n",
        "\n",
        "A partir de la mejor configuraci√≥n obtenida en la secci√≥n anterior, utilice `optuna` para optimizar sus hiperpar√°metros. En particular, se pide que su optimizaci√≥n considere lo siguiente:\n",
        "\n",
        "- Fijar una semilla en las instancias necesarias para garantizar la reproducibilidad de resultados\n",
        "- Utilice `TPESampler` como m√©todo de muestreo\n",
        "- De `XGBRegressor`, optimice los siguientes hiperpar√°metros:\n",
        "    - `learning_rate` buscando valores flotantes en el rango (0.001, 0.1)\n",
        "    - `n_estimators` buscando valores enteros en el rango (50, 1000)\n",
        "    - `max_depth` buscando valores enteros en el rango (3, 10)\n",
        "    - `max_leaves` buscando valores enteros en el rango (0, 100)\n",
        "    - `min_child_weight` buscando valores enteros en el rango (1, 5)\n",
        "    - `reg_alpha` buscando valores flotantes en el rango (0, 1)\n",
        "    - `reg_lambda` buscando valores flotantes en el rango (0, 1)\n",
        "- De `OneHotEncoder`, optimice el hiperpar√°metro `min_frequency` buscando el mejor valor flotante en el rango (0.0, 1.0)\n",
        "\n",
        "Para ello se pide los siguientes pasos:\n",
        "1. Implemente una funci√≥n `objective()` que permita minimizar el `MAE` en el conjunto de validaci√≥n. Use el m√©todo `.set_user_attr()` para almacenar el mejor pipeline entrenado. [10 puntos]\n",
        "2. Fije el tiempo de entrenamiento a 5 minutos. [1 punto]\n",
        "3. Optimizar el modelo y reportar el n√∫mero de *trials*, el `MAE` y los mejores hiperpar√°metros encontrados. ¬øC√≥mo cambian sus resultados con respecto a la secci√≥n anterior? ¬øA qu√© se puede deber esto? [3 puntos]\n",
        "4. Explique cada hiperpar√°metro y su rol en el modelo. ¬øHacen sentido los rangos de optimizaci√≥n indicados? [5 puntos]\n",
        "5. Guardar su modelo en un archivo .pkl [1 punto]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of trials: 485\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "# Inserte su c√≥digo ac√°\n",
        "\n",
        "def objective(trial):\n",
        "   learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.1)\n",
        "   n_estimators = trial.suggest_int(\"n_estimators\", 50, 1000)\n",
        "   max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
        "   max_leaves = trial.suggest_int(\"max_leaves\", 0, 100)\n",
        "   min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 5)\n",
        "   reg_alpha = trial.suggest_float(\"reg_alpha\", 0, 1)\n",
        "   reg_lambda = trial.suggest_float(\"reg_lambda\", 0, 1)\n",
        "   min_frequency = trial.suggest_float(\"min_frequency\", 0.0, 1.0)\n",
        "\n",
        "   pipeline_optuna = Pipeline(steps=[\n",
        "        ('date_transformer', date_transformer),\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', XGBRegressor(\n",
        "            learning_rate=learning_rate,\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=max_depth,\n",
        "            max_leaves=max_leaves,\n",
        "            min_child_weight=min_child_weight,\n",
        "            reg_alpha=reg_alpha,\n",
        "            reg_lambda=reg_lambda,\n",
        "            random_state=random_state  # Setting random seed\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "   pipeline_optuna.set_output(transform=\"pandas\")\n",
        "    # Train and evaluate\n",
        "   pipeline_optuna.fit(X_train, y_train)\n",
        "   predictions_optuna = pipeline_optuna.predict(X_validation)\n",
        "   mae_optuna = mean_absolute_error(y_validation, predictions_optuna)\n",
        "\n",
        "    #\n",
        "   trial.set_user_attr(\"best_pipeline\", pipeline_optuna)\n",
        "\n",
        "   return mae_optuna\n",
        "\n",
        "\n",
        "study1 = optuna.create_study(direction='minimize', sampler=TPESampler(seed=random_state))\n",
        "study1.optimize(objective, timeout=300)\n",
        "\n",
        "print(\"Number of trials:\", len(study1.trials))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best trial:\n",
            "  Value (MAE): 6528.5889\n",
            "  Params:\n",
            "    learning_rate: 0.04893953267834104\n",
            "    n_estimators: 254\n",
            "    max_depth: 5\n",
            "    max_leaves: 27\n",
            "    min_child_weight: 4\n",
            "    reg_alpha: 0.891587464952773\n",
            "    reg_lambda: 0.33735306944949706\n",
            "    min_frequency: 0.8222141714347942\n"
          ]
        }
      ],
      "source": [
        "print(\"Best trial:\")\n",
        "trial = study1.best_trial\n",
        "print(f\"  Value (MAE): {trial.value:.4f}\")\n",
        "print(\"  Params:\")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "# Get the best pipeline\n",
        "best_pipeline = trial.user_attrs[\"best_pipeline\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo guardado exitosamente.\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "with open('Optuna.pkl', 'wb') as file:\n",
        "    pickle.dump(best_pipeline, file)\n",
        "\n",
        "print(\"Modelo guardado exitosamente.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5195ccfc37e044ad9453f6eb2754f631",
        "deepnote_cell_type": "markdown",
        "id": "ZglyD_QWI5wA"
      },
      "source": [
        "## 4. Optimizaci√≥n de Hiperpar√°metros con Optuna y Prunners (17 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.pinimg.com/originals/90/16/f9/9016f919c2259f3d0e8fe465049638a7.gif\">\n",
        "</p>\n",
        "\n",
        "Despu√©s de optimizar el rendimiento de su modelo varias veces, Fiu le pregunta si no es posible optimizar el entrenamiento del modelo en s√≠ mismo. Despu√©s de leer un par de post de personas de dudosa reputaci√≥n en la *deepweb*, usted llega a la conclusi√≥n que puede cumplir este objetivo mediante la implementaci√≥n de **Prunning**.\n",
        "\n",
        "Vuelva a optimizar los mismos hiperpar√°metros que la secci√≥n pasada, pero esta vez utilizando **Prunning** en la optimizaci√≥n. En particular, usted debe:\n",
        "\n",
        "- Responder: ¬øQu√© es prunning? ¬øDe qu√© forma deber√≠a impactar en el entrenamiento? [2 puntos]\n",
        "- Redefinir la funci√≥n `objective()` utilizando `optuna.integration.XGBoostPruningCallback` como m√©todo de **Prunning** [10 puntos]\n",
        "- Fijar nuevamente el tiempo de entrenamiento a 5 minutos [1 punto]\n",
        "- Reportar el n√∫mero de *trials*, el `MAE` y los mejores hiperpar√°metros encontrados. ¬øC√≥mo cambian sus resultados con respecto a la secci√≥n anterior? ¬øA qu√© se puede deber esto? [3 puntos]\n",
        "- Guardar su modelo en un archivo .pkl [1 punto]\n",
        "\n",
        "Nota: Si quieren silenciar los prints obtenidos en el prunning, pueden hacerlo mediante el siguiente comando:\n",
        "\n",
        "```\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "```\n",
        "\n",
        "De implementar la opci√≥n anterior, pueden especificar `show_progress_bar = True` en el m√©todo `optimize` para *m√°s sabor*.\n",
        "\n",
        "Hint: Si quieren especificar par√°metros del m√©todo .fit() del modelo a trav√©s del pipeline, pueden hacerlo por medio de la siguiente sintaxis: `pipeline.fit(stepmodelo__parametro = valor)`\n",
        "\n",
        "Hint2: Este <a href = https://stackoverflow.com/questions/40329576/sklearn-pass-fit-parameters-to-xgboost-in-pipeline>enlace</a> les puede ser de ayuda en su implementaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IEEZnb-cwP_"
      },
      "outputs": [],
      "source": [
        "#!pip install optuna-integration[xgboost]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[W 2025-05-07 00:20:33,642] Trial 0 failed with parameters: {'learning_rate': 0.03807947176588889, 'n_estimators': 954, 'max_depth': 8, 'max_leaves': 60, 'min_child_weight': 1, 'reg_alpha': 0.15599452033620265, 'reg_lambda': 0.05808361216819946, 'min_frequency': 0.8661761457749352} because of the following error: XGBoostError(\"[00:20:33] /workspace/src/metric/metric.cc:49: Unknown metric function {'validation-mae': 'mae'}\\nStack trace:\\n  [bt] (0) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x25c1ac) [0x78b4a385c1ac]\\n  [bt] (1) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x62660e) [0x78b4a3c2660e]\\n  [bt] (2) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x5dd0bd) [0x78b4a3bdd0bd]\\n  [bt] (3) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x5f711d) [0x78b4a3bf711d]\\n  [bt] (4) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x5f8a2d) [0x78b4a3bf8a2d]\\n  [bt] (5) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x6f) [0x78b4a3765a1f]\\n  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x78b5100f8e2e]\\n  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x78b5100f5493]\\n  [bt] (8) /usr/lib/python3.11/lib-dynload/_ctypes.cpython-311-x86_64-linux-gnu.so(+0xa4d8) [0x78b5103594d8]\\n\\n\").\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-48-17b72f1e4931>\", line 61, in objective1\n",
            "    model.fit(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/xgboost/core.py\", line 726, in inner_f\n",
            "    return func(**kwargs)\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\", line 1170, in fit\n",
            "    self._Booster = train(\n",
            "                    ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/xgboost/core.py\", line 726, in inner_f\n",
            "    return func(**kwargs)\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/xgboost/training.py\", line 181, in train\n",
            "    bst.update(dtrain, iteration=i, fobj=obj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/xgboost/core.py\", line 2100, in update\n",
            "    _check_call(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/xgboost/core.py\", line 284, in _check_call\n",
            "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
            "xgboost.core.XGBoostError: [00:20:33] /workspace/src/metric/metric.cc:49: Unknown metric function {'validation-mae': 'mae'}\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x25c1ac) [0x78b4a385c1ac]\n",
            "  [bt] (1) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x62660e) [0x78b4a3c2660e]\n",
            "  [bt] (2) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x5dd0bd) [0x78b4a3bdd0bd]\n",
            "  [bt] (3) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x5f711d) [0x78b4a3bf711d]\n",
            "  [bt] (4) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x5f8a2d) [0x78b4a3bf8a2d]\n",
            "  [bt] (5) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x6f) [0x78b4a3765a1f]\n",
            "  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x78b5100f8e2e]\n",
            "  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x78b5100f5493]\n",
            "  [bt] (8) /usr/lib/python3.11/lib-dynload/_ctypes.cpython-311-x86_64-linux-gnu.so(+0xa4d8) [0x78b5103594d8]\n",
            "\n",
            "\n",
            "[W 2025-05-07 00:20:33,647] Trial 0 failed with value None.\n"
          ]
        },
        {
          "ename": "XGBoostError",
          "evalue": "[00:20:33] /workspace/src/metric/metric.cc:49: Unknown metric function {'validation-mae': 'mae'}\nStack trace:\n  [bt] (0) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x25c1ac) [0x78b4a385c1ac]\n  [bt] (1) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x62660e) [0x78b4a3c2660e]\n  [bt] (2) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x5dd0bd) [0x78b4a3bdd0bd]\n  [bt] (3) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x5f711d) [0x78b4a3bf711d]\n  [bt] (4) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x5f8a2d) [0x78b4a3bf8a2d]\n  [bt] (5) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x6f) [0x78b4a3765a1f]\n  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x78b5100f8e2e]\n  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x78b5100f5493]\n  [bt] (8) /usr/lib/python3.11/lib-dynload/_ctypes.cpython-311-x86_64-linux-gnu.so(+0xa4d8) [0x78b5103594d8]\n\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)\n",
            "\u001b[0;32m<ipython-input-48-17b72f1e4931>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n",
            "\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     72\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'minimize'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m---> 73\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     75\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of trials:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n",
            "\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    474\u001b[0m         \"\"\"\n",
            "\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n",
            "\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n",
            "\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n",
            "\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n",
            "\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n",
            "\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    247\u001b[0m     ):\n",
            "\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n",
            "\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;32m<ipython-input-48-17b72f1e4931>\u001b[0m in \u001b[0;36mobjective1\u001b[0;34m(trial)\u001b[0m\n",
            "\u001b[1;32m     59\u001b[0m     )\n",
            "\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m---> 61\u001b[0;31m     model.fit(\n",
            "\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mX_train_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Ensure the 'eval_set' is provided and correctly formatted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n",
            "\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m   1169\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_configure_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m-> 1170\u001b[0;31m             self._Booster = train(\n",
            "\u001b[0m\u001b[1;32m   1171\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m   1172\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n",
            "\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n",
            "\u001b[1;32m   2098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m   2099\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m-> 2100\u001b[0;31m             _check_call(\n",
            "\u001b[0m\u001b[1;32m   2101\u001b[0m                 _LIB.XGBoosterUpdateOneIter(\n",
            "\u001b[1;32m   2102\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n",
            "\u001b[1;32m    282\u001b[0m     \"\"\"\n",
            "\u001b[1;32m    283\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;31mXGBoostError\u001b[0m: [00:20:33] /workspace/src/metric/metric.cc:49: Unknown metric function {'validation-mae': 'mae'}\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x25c1ac) [0x78b4a385c1ac]\n",
            "  [bt] (1) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x62660e) [0x78b4a3c2660e]\n",
            "  [bt] (2) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x5dd0bd) [0x78b4a3bdd0bd]\n",
            "  [bt] (3) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x5f711d) [0x78b4a3bf711d]\n",
            "  [bt] (4) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x5f8a2d) [0x78b4a3bf8a2d]\n",
            "  [bt] (5) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x6f) [0x78b4a3765a1f]\n",
            "  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x78b5100f8e2e]\n",
            "  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x78b5100f5493]\n",
            "  [bt] (8) /usr/lib/python3.11/lib-dynload/_ctypes.cpython-311-x86_64-linux-gnu.so(+0xa4d8) [0x78b5103594d8]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from optuna.integration import XGBoostPruningCallback\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "random_state = 42\n",
        "\n",
        "def objective1(trial):\n",
        "    # Hiperpar√°metros\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.1)\n",
        "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 1000)\n",
        "    max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
        "    max_leaves = trial.suggest_int(\"max_leaves\", 0, 100)\n",
        "    min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 5)\n",
        "    reg_alpha = trial.suggest_float(\"reg_alpha\", 0, 1)\n",
        "    reg_lambda = trial.suggest_float(\"reg_lambda\", 0, 1)\n",
        "    min_frequency = trial.suggest_float(\"min_frequency\", 0.0, 1.0)\n",
        "\n",
        "    date_pipeline = Pipeline(steps=[\n",
        "        ('date_transformer', date_transformer)\n",
        "    ])\n",
        "    X_train_trans = date_pipeline.fit_transform(X_train)\n",
        "    X_val_trans = date_pipeline.transform(X_validation)\n",
        "\n",
        "    categorical_features_updated = X_train_trans.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), numerical_columns),\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False, min_frequency=min_frequency), categorical_features_updated)\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "    ).set_output(transform=\"pandas\")\n",
        "\n",
        "    X_train_final = preprocessor.fit_transform(X_train_trans)\n",
        "    X_val_final = preprocessor.transform(X_val_trans)\n",
        "\n",
        "    # Modelo con pruning\n",
        "    model = XGBRegressor(\n",
        "        learning_rate=learning_rate,\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        max_leaves=max_leaves,\n",
        "        min_child_weight=min_child_weight,\n",
        "        reg_alpha=reg_alpha,\n",
        "        reg_lambda=reg_lambda,\n",
        "        random_state=random_state,\n",
        "        enable_categorical=True,\n",
        "        # The change involves passing a dictionary with the desired evaluation metric and name.\n",
        "        # By using {'validation-mae': 'mae'}, we explicitly name the metric and set how it should be calculated.\n",
        "        eval_metric={'validation-mae': 'mae'},\n",
        "        callbacks=[XGBoostPruningCallback(trial, \"validation-mae\")]\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_train_final, y_train,\n",
        "        # Ensure the 'eval_set' is provided and correctly formatted.\n",
        "        eval_set=[(X_val_final, y_validation)],\n",
        "        verbose=False,\n",
        "    )\n",
        "\n",
        "    preds = model.predict(X_val_final)\n",
        "    mae = mean_absolute_error(y_validation, preds)\n",
        "    return mae\n",
        "\n",
        "study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=random_state))\n",
        "study.optimize(objective1, timeout=300)\n",
        "\n",
        "print(\"Number of trials:\", len(study.trials))\n",
        "\n",
        "study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=random_state))\n",
        "study.optimize(objective1, timeout=300)\n",
        "\n",
        "print(\"Number of trials:\", len(study.trials))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8a081778cc704fc6bed05393a5419327",
        "deepnote_cell_type": "markdown",
        "id": "ZMiiVaCUI5wA"
      },
      "source": [
        "## 5. Visualizaciones (5 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/F-LgB1xTebEAAAAd/look-at-this-graph-nickelback.gif\">\n",
        "</p>\n",
        "\n",
        "\n",
        "Satisfecho con su trabajo, Fiu le pregunta si es posible generar visualizaciones que permitan entender el entrenamiento de su modelo.\n",
        "\n",
        "A partir del siguiente <a href = https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/005_visualization.html#visualization>enlace</a>, genere las siguientes visualizaciones:\n",
        "\n",
        "1. Gr√°fico de historial de optimizaci√≥n [1 punto]\n",
        "2. Gr√°fico de coordenadas paralelas [1 punto]\n",
        "3. Gr√°fico de importancia de hiperpar√°metros [1 punto]\n",
        "\n",
        "Comente sus resultados:\n",
        "\n",
        "4. ¬øDesde qu√© *trial* se empiezan a observar mejoras notables en sus resultados? [0.5 puntos]\n",
        "5. ¬øQu√© tendencias puede observar a partir del gr√°fico de coordenadas paralelas? [1 punto]\n",
        "6. ¬øCu√°les son los hiperpar√°metros con mayor importancia para la optimizaci√≥n de su modelo? [0.5 puntos]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"7f28469a-9ab2-4c31-8bcd-1f37cfe848f0\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"7f28469a-9ab2-4c31-8bcd-1f37cfe848f0\")) {                    Plotly.newPlot(                        \"7f28469a-9ab2-4c31-8bcd-1f37cfe848f0\",                        [{\"mode\":\"markers\",\"name\":\"Objective Value\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484],\"y\":[7007.95703125,6637.349609375,6610.51220703125,6656.84765625,6863.22119140625,7395.537109375,6867.76806640625,8673.1630859375,6613.5908203125,8707.541015625,13298.498046875,6590.04833984375,6548.84375,6626.29833984375,6914.8203125,6751.91259765625,6829.97900390625,6625.62109375,6604.37939453125,6727.705078125,6605.10595703125,6594.95703125,6600.333984375,6638.56494140625,6590.1220703125,6650.185546875,6583.19384765625,7634.36669921875,6667.81884765625,7166.736328125,6686.90478515625,6608.29833984375,6559.06494140625,6789.44189453125,6679.92333984375,6641.423828125,6718.26171875,6787.703125,6616.7685546875,6703.5595703125,7266.310546875,6595.31982421875,6594.42431640625,6622.9482421875,6650.20947265625,7067.29296875,6749.18310546875,8432.1220703125,7053.7265625,6619.82421875,6785.34521484375,6593.25341796875,6645.6728515625,6641.09033203125,6579.16455078125,6563.669921875,6623.27685546875,6682.27490234375,6651.50390625,7146.3701171875,6644.376953125,6580.2744140625,6604.23583984375,8375.7509765625,6607.9248046875,6558.736328125,6635.4072265625,6631.64794921875,6640.908203125,6590.23583984375,6603.72216796875,6627.794921875,6597.07568359375,6595.22314453125,6591.6123046875,6609.59619140625,6776.685546875,6601.31982421875,6615.09326171875,6602.89990234375,6746.00390625,6679.05517578125,6666.22607421875,6629.84423828125,6599.34521484375,6660.59228515625,6618.35546875,6706.54541015625,6655.7158203125,6591.0263671875,7853.14697265625,6582.80126953125,6601.70068359375,6605.2529296875,6683.19384765625,6565.310546875,6610.626953125,6616.51171875,6584.5380859375,6610.31396484375,6644.37841796875,6680.05029296875,6543.23828125,6719.845703125,6587.00927734375,6701.169921875,6609.30322265625,6608.14404296875,6572.1025390625,6636.12890625,6619.3857421875,6588.419921875,6556.26611328125,6528.5888671875,6575.0986328125,6665.62158203125,6702.54443359375,6715.271484375,6584.46875,6585.62841796875,6631.24267578125,6629.72119140625,6607.30712890625,6590.86767578125,6625.8095703125,6600.49169921875,6595.5185546875,6710.02392578125,6592.51220703125,6624.1279296875,6609.70361328125,6610.90185546875,6625.0068359375,6749.16455078125,6577.2158203125,6590.9765625,6627.12353515625,6613.42431640625,6600.28173828125,6609.42431640625,6655.67919921875,6632.33203125,6809.95458984375,6593.06640625,6599.29931640625,6590.09130859375,6679.12548828125,6666.6103515625,6588.82275390625,6609.87646484375,6660.3017578125,6587.96630859375,6614.4931640625,6576.681640625,6625.94970703125,6572.5146484375,6575.1962890625,6568.24462890625,6593.33935546875,6607.05419921875,6568.814453125,6596.18505859375,6631.07958984375,6597.71484375,6615.2060546875,6607.01953125,6578.537109375,6593.20947265625,6829.83154296875,6590.26025390625,6604.158203125,6589.60009765625,6614.46826171875,6573.06982421875,6612.64208984375,6624.3447265625,6591.3388671875,6589.263671875,6661.00927734375,6630.7216796875,6578.18896484375,6593.51171875,6616.89208984375,6597.83349609375,6609.42236328125,6589.16845703125,6597.66455078125,6637.5048828125,6686.10400390625,6614.4375,6588.76611328125,6561.1044921875,6600.64501953125,6609.5830078125,6593.62060546875,6567.78955078125,6594.572265625,6576.23388671875,6606.31201171875,6635.32275390625,6614.4814453125,6588.06982421875,6621.392578125,6606.3701171875,6593.89794921875,6592.52783203125,6591.3642578125,6620.32080078125,6574.45556640625,6578.5966796875,6581.85107421875,6582.421875,6599.54345703125,6574.74365234375,6532.60009765625,6596.35546875,6583.041015625,6573.2919921875,6597.62060546875,6613.6943359375,6582.4345703125,6594.70703125,6600.59912109375,6590.03955078125,6591.93115234375,6571.4482421875,6612.27294921875,6587.837890625,6550.25732421875,6578.9228515625,6622.44677734375,6609.84033203125,6598.27783203125,6625.7021484375,6591.974609375,6609.1005859375,6576.310546875,6537.71484375,6600.22607421875,6590.798828125,6609.63330078125,7805.06298828125,6583.7060546875,6671.68603515625,6543.3603515625,6577.779296875,6558.150390625,6569.7392578125,6572.38623046875,6593.24462890625,6586.2919921875,6581.19189453125,6638.31005859375,6784.13818359375,6605.94384765625,6592.0595703125,6606.14599609375,6554.6279296875,6582.47216796875,6536.71240234375,6591.830078125,6577.14306640625,6576.5849609375,6592.8515625,6620.37548828125,6562.462890625,6641.93408203125,6591.19384765625,6631.07177734375,6853.306640625,6576.81103515625,6580.25,6602.25,6584.150390625,6691.3173828125,6593.04296875,6568.3662109375,10161.9873046875,6587.533203125,6621.29052734375,6749.8271484375,6571.333984375,6666.78857421875,6604.47802734375,6733.06787109375,6662.67138671875,6797.69287109375,6904.7841796875,6888.63427734375,6591.7197265625,6704.88134765625,6556.552734375,6563.73974609375,6601.4072265625,6595.5009765625,6597.5517578125,6629.384765625,6715.6103515625,6588.869140625,6635.31591796875,6597.98779296875,6642.17822265625,6571.560546875,6588.05078125,6608.162109375,6612.2158203125,6675.58203125,6634.9697265625,6676.99951171875,6603.1181640625,6642.89111328125,6590.1259765625,6585.48486328125,6554.6171875,6564.53515625,6566.59033203125,6584.2841796875,6644.0390625,6585.470703125,6604.2255859375,6620.7236328125,6621.93408203125,7560.08984375,6579.1162109375,6577.96826171875,6566.1396484375,6596.28173828125,6587.529296875,6583.46337890625,6586.00927734375,6579.845703125,6597.04638671875,6572.91796875,6577.1904296875,6785.4033203125,6591.53173828125,6666.28369140625,6603.326171875,6711.982421875,8425.92578125,6803.56884765625,6568.7509765625,7198.23193359375,6625.451171875,6645.29052734375,6731.42724609375,6706.224609375,6576.3369140625,6625.2783203125,6557.51318359375,6589.97314453125,6545.830078125,6667.28515625,6563.29052734375,6592.4697265625,6600.72314453125,6591.86328125,6580.67626953125,6570.4833984375,6661.15966796875,6620.35791015625,6573.8583984375,6619.08251953125,6570.7041015625,6592.2978515625,6607.00341796875,6656.5634765625,6580.39794921875,6605.419921875,6563.88134765625,6546.80224609375,6564.86376953125,6572.33056640625,6659.5078125,6583.82958984375,6553.6767578125,6629.984375,6677.3466796875,6569.54638671875,6576.87744140625,6595.6318359375,6602.974609375,6583.3896484375,6618.5927734375,6590.24609375,6585.7158203125,6608.4306640625,6577.91748046875,6600.65673828125,6590.71240234375,6593.2099609375,6774.93896484375,6583.18017578125,6581.68994140625,6668.25146484375,6670.81982421875,6605.7744140625,6744.1220703125,6786.28955078125,6578.77001953125,6615.0234375,6598.29296875,6620.68896484375,7956.79296875,6570.2587890625,6626.36474609375,6591.9150390625,6597.001953125,6567.3291015625,6619.43994140625,6622.63427734375,6618.7841796875,6644.69091796875,6646.8974609375,6709.55126953125,6612.52099609375,6618.53271484375,6569.0732421875,6634.6982421875,6611.7265625,6691.50927734375,6585.9482421875,6655.29052734375,6621.0400390625,6586.20654296875,6646.63037109375,6795.59619140625,6610.8505859375,6582.2255859375,6606.8408203125,6630.30908203125,6603.73388671875,6623.03564453125,6602.6357421875,6641.45361328125,6622.33935546875,6626.79541015625,6691.10107421875,6582.72119140625,6662.9873046875,6641.14404296875,6573.1650390625,6591.7373046875,6529.39794921875,6644.50048828125,6584.35546875,6587.0166015625,6633.24755859375,6704.99755859375,6617.91015625,6588.4921875,6660.03466796875,6621.1162109375,6548.83544921875,6565.10400390625,6558.2626953125,6573.3251953125,6587.18310546875,6571.353515625,6587.64208984375,6605.22265625,6599.490234375,6596.361328125,6590.0400390625,6584.62255859375,6574.0068359375,6612.748046875,6604.1650390625,6595.9501953125,6611.99755859375,6625.53662109375,6587.25439453125,6575.4794921875,6658.87451171875,6718.22998046875,6632.5498046875,6562.4833984375,6619.6513671875,6589.77587890625,6645.5546875,6583.0517578125,6588.24462890625,6582.66845703125,6679.640625,6680.68798828125],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Best Value\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484],\"y\":[7007.95703125,6637.349609375,6610.51220703125,6610.51220703125,6610.51220703125,6610.51220703125,6610.51220703125,6610.51220703125,6610.51220703125,6610.51220703125,6610.51220703125,6590.04833984375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6548.84375,6543.23828125,6543.23828125,6543.23828125,6543.23828125,6543.23828125,6543.23828125,6543.23828125,6543.23828125,6543.23828125,6543.23828125,6543.23828125,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875,6528.5888671875],\"type\":\"scatter\"},{\"marker\":{\"color\":\"#cccccc\"},\"mode\":\"markers\",\"name\":\"Infeasible Trial\",\"showlegend\":false,\"x\":[],\"y\":[],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"Optimization History Plot\"},\"xaxis\":{\"title\":{\"text\":\"Trial\"}},\"yaxis\":{\"title\":{\"text\":\"Objective Value\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('7f28469a-9ab2-4c31-8bcd-1f37cfe848f0');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = optuna.visualization.plot_optimization_history(study1)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"ef44c656-4a9b-4fbe-9f55-8b3d54d25ba2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ef44c656-4a9b-4fbe-9f55-8b3d54d25ba2\")) {                    Plotly.newPlot(                        \"ef44c656-4a9b-4fbe-9f55-8b3d54d25ba2\",                        [{\"dimensions\":[{\"label\":\"Objective Value\",\"range\":[6528.5888671875,13298.498046875],\"values\":[7007.95703125,6637.349609375,6610.51220703125,6656.84765625,6863.22119140625,7395.537109375,6867.76806640625,8673.1630859375,6613.5908203125,8707.541015625,13298.498046875,6590.04833984375,6548.84375,6626.29833984375,6914.8203125,6751.91259765625,6829.97900390625,6625.62109375,6604.37939453125,6727.705078125,6605.10595703125,6594.95703125,6600.333984375,6638.56494140625,6590.1220703125,6650.185546875,6583.19384765625,7634.36669921875,6667.81884765625,7166.736328125,6686.90478515625,6608.29833984375,6559.06494140625,6789.44189453125,6679.92333984375,6641.423828125,6718.26171875,6787.703125,6616.7685546875,6703.5595703125,7266.310546875,6595.31982421875,6594.42431640625,6622.9482421875,6650.20947265625,7067.29296875,6749.18310546875,8432.1220703125,7053.7265625,6619.82421875,6785.34521484375,6593.25341796875,6645.6728515625,6641.09033203125,6579.16455078125,6563.669921875,6623.27685546875,6682.27490234375,6651.50390625,7146.3701171875,6644.376953125,6580.2744140625,6604.23583984375,8375.7509765625,6607.9248046875,6558.736328125,6635.4072265625,6631.64794921875,6640.908203125,6590.23583984375,6603.72216796875,6627.794921875,6597.07568359375,6595.22314453125,6591.6123046875,6609.59619140625,6776.685546875,6601.31982421875,6615.09326171875,6602.89990234375,6746.00390625,6679.05517578125,6666.22607421875,6629.84423828125,6599.34521484375,6660.59228515625,6618.35546875,6706.54541015625,6655.7158203125,6591.0263671875,7853.14697265625,6582.80126953125,6601.70068359375,6605.2529296875,6683.19384765625,6565.310546875,6610.626953125,6616.51171875,6584.5380859375,6610.31396484375,6644.37841796875,6680.05029296875,6543.23828125,6719.845703125,6587.00927734375,6701.169921875,6609.30322265625,6608.14404296875,6572.1025390625,6636.12890625,6619.3857421875,6588.419921875,6556.26611328125,6528.5888671875,6575.0986328125,6665.62158203125,6702.54443359375,6715.271484375,6584.46875,6585.62841796875,6631.24267578125,6629.72119140625,6607.30712890625,6590.86767578125,6625.8095703125,6600.49169921875,6595.5185546875,6710.02392578125,6592.51220703125,6624.1279296875,6609.70361328125,6610.90185546875,6625.0068359375,6749.16455078125,6577.2158203125,6590.9765625,6627.12353515625,6613.42431640625,6600.28173828125,6609.42431640625,6655.67919921875,6632.33203125,6809.95458984375,6593.06640625,6599.29931640625,6590.09130859375,6679.12548828125,6666.6103515625,6588.82275390625,6609.87646484375,6660.3017578125,6587.96630859375,6614.4931640625,6576.681640625,6625.94970703125,6572.5146484375,6575.1962890625,6568.24462890625,6593.33935546875,6607.05419921875,6568.814453125,6596.18505859375,6631.07958984375,6597.71484375,6615.2060546875,6607.01953125,6578.537109375,6593.20947265625,6829.83154296875,6590.26025390625,6604.158203125,6589.60009765625,6614.46826171875,6573.06982421875,6612.64208984375,6624.3447265625,6591.3388671875,6589.263671875,6661.00927734375,6630.7216796875,6578.18896484375,6593.51171875,6616.89208984375,6597.83349609375,6609.42236328125,6589.16845703125,6597.66455078125,6637.5048828125,6686.10400390625,6614.4375,6588.76611328125,6561.1044921875,6600.64501953125,6609.5830078125,6593.62060546875,6567.78955078125,6594.572265625,6576.23388671875,6606.31201171875,6635.32275390625,6614.4814453125,6588.06982421875,6621.392578125,6606.3701171875,6593.89794921875,6592.52783203125,6591.3642578125,6620.32080078125,6574.45556640625,6578.5966796875,6581.85107421875,6582.421875,6599.54345703125,6574.74365234375,6532.60009765625,6596.35546875,6583.041015625,6573.2919921875,6597.62060546875,6613.6943359375,6582.4345703125,6594.70703125,6600.59912109375,6590.03955078125,6591.93115234375,6571.4482421875,6612.27294921875,6587.837890625,6550.25732421875,6578.9228515625,6622.44677734375,6609.84033203125,6598.27783203125,6625.7021484375,6591.974609375,6609.1005859375,6576.310546875,6537.71484375,6600.22607421875,6590.798828125,6609.63330078125,7805.06298828125,6583.7060546875,6671.68603515625,6543.3603515625,6577.779296875,6558.150390625,6569.7392578125,6572.38623046875,6593.24462890625,6586.2919921875,6581.19189453125,6638.31005859375,6784.13818359375,6605.94384765625,6592.0595703125,6606.14599609375,6554.6279296875,6582.47216796875,6536.71240234375,6591.830078125,6577.14306640625,6576.5849609375,6592.8515625,6620.37548828125,6562.462890625,6641.93408203125,6591.19384765625,6631.07177734375,6853.306640625,6576.81103515625,6580.25,6602.25,6584.150390625,6691.3173828125,6593.04296875,6568.3662109375,10161.9873046875,6587.533203125,6621.29052734375,6749.8271484375,6571.333984375,6666.78857421875,6604.47802734375,6733.06787109375,6662.67138671875,6797.69287109375,6904.7841796875,6888.63427734375,6591.7197265625,6704.88134765625,6556.552734375,6563.73974609375,6601.4072265625,6595.5009765625,6597.5517578125,6629.384765625,6715.6103515625,6588.869140625,6635.31591796875,6597.98779296875,6642.17822265625,6571.560546875,6588.05078125,6608.162109375,6612.2158203125,6675.58203125,6634.9697265625,6676.99951171875,6603.1181640625,6642.89111328125,6590.1259765625,6585.48486328125,6554.6171875,6564.53515625,6566.59033203125,6584.2841796875,6644.0390625,6585.470703125,6604.2255859375,6620.7236328125,6621.93408203125,7560.08984375,6579.1162109375,6577.96826171875,6566.1396484375,6596.28173828125,6587.529296875,6583.46337890625,6586.00927734375,6579.845703125,6597.04638671875,6572.91796875,6577.1904296875,6785.4033203125,6591.53173828125,6666.28369140625,6603.326171875,6711.982421875,8425.92578125,6803.56884765625,6568.7509765625,7198.23193359375,6625.451171875,6645.29052734375,6731.42724609375,6706.224609375,6576.3369140625,6625.2783203125,6557.51318359375,6589.97314453125,6545.830078125,6667.28515625,6563.29052734375,6592.4697265625,6600.72314453125,6591.86328125,6580.67626953125,6570.4833984375,6661.15966796875,6620.35791015625,6573.8583984375,6619.08251953125,6570.7041015625,6592.2978515625,6607.00341796875,6656.5634765625,6580.39794921875,6605.419921875,6563.88134765625,6546.80224609375,6564.86376953125,6572.33056640625,6659.5078125,6583.82958984375,6553.6767578125,6629.984375,6677.3466796875,6569.54638671875,6576.87744140625,6595.6318359375,6602.974609375,6583.3896484375,6618.5927734375,6590.24609375,6585.7158203125,6608.4306640625,6577.91748046875,6600.65673828125,6590.71240234375,6593.2099609375,6774.93896484375,6583.18017578125,6581.68994140625,6668.25146484375,6670.81982421875,6605.7744140625,6744.1220703125,6786.28955078125,6578.77001953125,6615.0234375,6598.29296875,6620.68896484375,7956.79296875,6570.2587890625,6626.36474609375,6591.9150390625,6597.001953125,6567.3291015625,6619.43994140625,6622.63427734375,6618.7841796875,6644.69091796875,6646.8974609375,6709.55126953125,6612.52099609375,6618.53271484375,6569.0732421875,6634.6982421875,6611.7265625,6691.50927734375,6585.9482421875,6655.29052734375,6621.0400390625,6586.20654296875,6646.63037109375,6795.59619140625,6610.8505859375,6582.2255859375,6606.8408203125,6630.30908203125,6603.73388671875,6623.03564453125,6602.6357421875,6641.45361328125,6622.33935546875,6626.79541015625,6691.10107421875,6582.72119140625,6662.9873046875,6641.14404296875,6573.1650390625,6591.7373046875,6529.39794921875,6644.50048828125,6584.35546875,6587.0166015625,6633.24755859375,6704.99755859375,6617.91015625,6588.4921875,6660.03466796875,6621.1162109375,6548.83544921875,6565.10400390625,6558.2626953125,6573.3251953125,6587.18310546875,6571.353515625,6587.64208984375,6605.22265625,6599.490234375,6596.361328125,6590.0400390625,6584.62255859375,6574.0068359375,6612.748046875,6604.1650390625,6595.9501953125,6611.99755859375,6625.53662109375,6587.25439453125,6575.4794921875,6658.87451171875,6718.22998046875,6632.5498046875,6562.4833984375,6619.6513671875,6589.77587890625,6645.5546875,6583.0517578125,6588.24462890625,6582.66845703125,6679.640625,6680.68798828125]},{\"label\":\"learning_rate\",\"range\":[0.0015466895952366377,0.09993005423129347],\"values\":[0.03807947176588889,0.06051038616257767,0.03111998205299424,0.04615092843748656,0.0074401077055426725,0.013081785249633104,0.05512431765498469,0.00976075770314003,0.028812516459050697,0.0015466895952366377,0.08673802520395674,0.02965374495889686,0.031804109333339874,0.07041509694398221,0.023242652390352673,0.043296510494269316,0.020030584157440107,0.07323157228691363,0.019488246255089475,0.038087397557478725,0.054965590491080754,0.02034951737088639,0.032626011572554764,0.01943537906727733,0.0288018991366208,0.09993005423129347,0.039330251816641834,0.0429776213753238,0.05069489459109856,0.036418283154438486,0.06760483065664369,0.027954635379339173,0.03912739657391358,0.03661922660910376,0.04753342094769683,0.04060100387395845,0.05970395011315449,0.05075084824663494,0.032824484928479294,0.026216679752352492,0.014664524989361598,0.03378815198697763,0.026871572896779286,0.045079578434642255,0.03947585961350976,0.01423325114042082,0.030080495426425165,0.0072662052934039725,0.024090631136134227,0.04751071364913585,0.03433189856645561,0.02704141252720649,0.029916183901220153,0.022713205432944226,0.03981461284264527,0.041767427262968206,0.05542787967629148,0.04132696670908376,0.037761750209713466,0.04576964836633554,0.055008129659658324,0.035134272052124285,0.04156661861531186,0.03548676889951172,0.03188903853541016,0.0512490784439391,0.04977876558900301,0.06122801187744406,0.04331147849334533,0.051930179018357746,0.06150899541774501,0.038648953677727645,0.0438783588961256,0.037085912001967127,0.039817212317333296,0.04827172610752964,0.0349605939689658,0.052479776595264,0.058313847276765175,0.031053108015546102,0.07798717411126141,0.017477625797263897,0.024671946351281303,0.02993092793245132,0.03277418567658627,0.04497291760815504,0.02153519741235556,0.0281351782474669,0.040995103375819755,0.036306971455036995,0.025007300133741785,0.05363259475517225,0.047191478760020396,0.056804852988844294,0.05300246002232449,0.04200063506513911,0.06434023710168554,0.04256187091812134,0.03868233232623586,0.049223910701494275,0.03909279052849894,0.034209524717252124,0.04595779022530317,0.045757621274125825,0.04421915617474288,0.04140272506471928,0.051024518954868744,0.03788979517941758,0.04273984128276938,0.04643742068751357,0.05367646349849487,0.03923013049950341,0.043347207940275884,0.04893953267834104,0.04856834639148545,0.04882635956260251,0.042383606307233694,0.09873643655755635,0.04370915748187732,0.04678756404073848,0.050771736490307175,0.056611802714307505,0.04891087671208989,0.054154125050200286,0.035894659631412515,0.04551391030237777,0.042014620916473545,0.04767616768471665,0.044403170714247736,0.04064951782242887,0.058555617626189375,0.03727201055514083,0.04020679011484254,0.03197948108801327,0.03376620478876371,0.03465688771315882,0.05138862160449739,0.043355635509658645,0.03270688007385144,0.03642111785380986,0.04948007527989268,0.040164521999007344,0.04543800816687155,0.03793929920616907,0.042308308300528355,0.03489812467767383,0.05233130532006119,0.03048464480517298,0.04725778302006488,0.0393114799238883,0.04385724125182338,0.042208015608642235,0.044941390299408863,0.04984496952652781,0.05494543743022715,0.04963297254227585,0.05101735600878481,0.050525965722624855,0.050650947086560504,0.04899471155668191,0.04719723930588972,0.04807733428688111,0.04640482679185578,0.05279472179677861,0.05015742684592737,0.05594095749708472,0.047218392058214154,0.047308563588694494,0.05162477730717984,0.049208816467890104,0.04612059715231621,0.043827053340331754,0.048492760963720634,0.04587410411721415,0.052415534314008555,0.05038708849215589,0.046122950416186685,0.05434856071678712,0.04785478887990345,0.04525444923679889,0.05042586373351222,0.05006060665350224,0.052118836613736765,0.0428649705023057,0.04750151135610243,0.05802054001785494,0.04467084998056469,0.04927497485148224,0.04142389592374827,0.0512568584633305,0.054688491114806816,0.041038280722099585,0.04688866186515854,0.04365864776203726,0.04112697892319605,0.04546503310819969,0.04491413365936824,0.0428333780556182,0.04196474560526564,0.03947263533257002,0.037776996334056547,0.04555728529597065,0.04937143740553504,0.04284413738283352,0.04848970864544014,0.04464229613208944,0.04055892120482246,0.05341794269332466,0.05136618778847005,0.043375059497498904,0.04661978608971013,0.05059315149939214,0.05247215023467808,0.05068889210703129,0.04845241502783446,0.048225099502604264,0.046220446047089574,0.05116941189174914,0.055945846013740745,0.05285922569843916,0.048031908416913245,0.04890548836045904,0.05158139384743277,0.0502777168257574,0.0454468003341237,0.05327787633822403,0.05325325392314743,0.05715080368855767,0.054163167787687885,0.0545085226447192,0.05566476772800549,0.05181307093952517,0.0605443918170208,0.04320871750396107,0.04691753790627798,0.05111110687927312,0.04831891532748582,0.05372832862498797,0.05552701190712466,0.05341823236753725,0.05082896856224584,0.053469958899894726,0.041723209617368355,0.04466004509599906,0.049316598780972824,0.05000205080471844,0.05219728837586318,0.05376615263725166,0.057727808375817466,0.06468563533993106,0.055671340886041164,0.05703076994388721,0.05370705914047559,0.059165689953079394,0.054628415371360435,0.0577169161134753,0.062388310635520576,0.05289862070282699,0.05301141680963835,0.05467471508106142,0.05550941435778622,0.059960931812177355,0.0582010461460162,0.05450214169914285,0.04646255957246335,0.05752783001431373,0.05829045911447673,0.05681112313160928,0.06003925888848418,0.05299344341989009,0.05573269845137364,0.054145536353664835,0.039521064412843955,0.056340732113411895,0.07354487106193167,0.048690643512712364,0.05329491778800023,0.0025067794921939737,0.0574033734829886,0.052443054959546986,0.09026668223055,0.05390887739718553,0.053062888111853994,0.04089295890641672,0.05220041787227651,0.054767704588166426,0.03740293220335238,0.049639150696616376,0.04358940732951727,0.05403198889754457,0.051422150459523044,0.04666345569207999,0.046847756114506846,0.046682354226630605,0.0479547869373786,0.045273624137357936,0.047973564227264946,0.04438733019115528,0.0498605584246211,0.04661206928379113,0.04899031780865206,0.04157970745173079,0.05195207998910337,0.04423740753389646,0.0474970718796298,0.055665359755821125,0.018335552694132187,0.050614632709049534,0.028120485499981224,0.04916164192313619,0.03949200820694207,0.06293602149386075,0.04536922799598495,0.05171137576847705,0.047471711433261496,0.04711623693278766,0.04286507161313968,0.046026615603130865,0.04803387309140919,0.05066345263801823,0.04448002657429052,0.04171424876346569,0.0491178106562859,0.047577703997697354,0.05159693380984445,0.04563015450438642,0.04526094510598283,0.04304404315265798,0.04612303654626975,0.04353318830429417,0.03810747294460174,0.04796097219547808,0.040229982369415655,0.04655611034816051,0.0493493749178182,0.04192544585661153,0.045170992067301484,0.049756833701483004,0.04679185239673859,0.011434353814509114,0.022738214577937146,0.04360104726341158,0.04039025752099695,0.048155218222119336,0.04963000888023904,0.03607346926805701,0.05159604131471479,0.045321192327796224,0.042814348312803806,0.047782282817142446,0.04588030749031554,0.047514969521676303,0.026604474993247352,0.04727392958518295,0.04805902555222852,0.05121816437767621,0.04385230559241156,0.049790929007121766,0.05222873614078886,0.0390064756303822,0.046955526195052194,0.05528971385885898,0.041234471785052004,0.05912802423714894,0.048869631509573845,0.04441945025892791,0.05229881335807561,0.04776287405229875,0.050298394383594625,0.041718069068620356,0.038577653081552,0.039030827493931955,0.03525031749350099,0.033023950278347376,0.037606566269679606,0.041207690102951645,0.0362805745656149,0.029250796129521844,0.04019630302825297,0.041352401309836556,0.03763069442691312,0.030970662291305514,0.03802834266099091,0.05654959322047353,0.04176242763908132,0.039882070444277384,0.05447640466647603,0.04232270943975142,0.044375294395436096,0.040027846994965866,0.052601941817315756,0.08729254203042644,0.03571458113000213,0.042957020929845116,0.05590679897166612,0.05052672576809236,0.05434951492527397,0.06050608088708009,0.03324086712629664,0.03868604735785917,0.05176874800081834,0.05759573196255693,0.04422858378140547,0.04950410359152191,0.04123190299947982,0.04632733669400411,0.04906365231302871,0.052556565356196794,0.0433037731597285,0.04729868073246392,0.050847917891440476,0.05452751890050423,0.02471243305128821,0.045299528430075515,0.03740964267299512,0.04090190240104603,0.05675094793892374,0.04776708884646568,0.07942600653482028,0.052895131349822405,0.042611435941463445,0.04960574277840771,0.058973418301075525,0.044605485299253275,0.03468922918158358,0.039613463753058936,0.05121770750513325,0.046363295413764535,0.05444544019996041,0.04877079139790525,0.04212015510974578,0.04401902601937724,0.05244376795198063,0.04647832127101619,0.04867243044600978,0.05602120448019923,0.050558219732477774,0.036341841595553745,0.041029643802731254,0.053549921145323,0.03834435155750539,0.04526578589378978,0.04836306604452667,0.05112046063271722,0.05132506780781779,0.05308208577653345,0.05030847292784685,0.055509013845523314,0.05820849890210363,0.05190310661197377,0.049456306519923554,0.046877841179357255,0.06152259054592944,0.053784172803500147,0.05488071652003089,0.05688993962552062,0.057647318515723085,0.059969498447504654,0.05656783740400511,0.05475226916543038,0.05741186799231082,0.05275344921851165,0.053823018553645234,0.054012870235440213,0.06349380257769051,0.05557296468688888,0.05936664464013907,0.05213799304999462,0.05698126666372714,0.05070562341647305,0.05305977026226978,0.05055055043924031,0.05592701227727083,0.09456376125796433,0.03168806365949533,0.052031684738420216,0.05848976701492944,0.05978439720750758,0.06047764840198312,0.0572170166900082,0.06684967259468347,0.05499012466462076,0.05852550038376497,0.06551858056931467,0.058694812705462784]},{\"label\":\"max_depth\",\"range\":[3,10],\"values\":[8,3,6,4,10,3,10,3,4,8,6,5,6,5,7,5,7,5,8,6,4,8,9,7,9,9,5,5,6,4,5,6,7,7,6,5,7,6,4,3,5,9,10,9,8,6,4,5,7,10,8,10,10,10,9,9,9,5,6,8,9,9,9,8,9,5,8,7,4,6,9,5,5,5,4,6,3,5,7,5,6,9,9,8,9,9,4,10,5,8,10,6,6,6,7,5,5,5,6,6,6,6,5,5,6,5,4,6,5,5,5,5,5,5,5,5,4,5,5,5,5,5,5,4,5,5,4,7,9,6,5,5,5,5,5,9,5,4,5,6,9,5,5,5,5,5,6,5,5,8,6,5,5,5,5,9,9,9,9,9,9,9,9,9,9,9,9,9,10,9,9,9,9,9,9,8,10,9,9,8,9,9,9,9,9,5,5,9,5,9,5,9,9,9,9,9,5,8,8,7,8,9,9,7,5,9,9,5,5,5,5,5,5,5,5,5,5,5,5,5,7,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,9,5,4,5,5,5,5,5,9,9,9,9,9,9,9,9,9,3,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,10,9,9,9,9,9,9,9,9,9,9,9,9,10,9,9,9,9,9,4,9,9,9,9,9,9,9,9,8,9,9,10,9,9,9,9,8,9,9,9,9,6,7,9,9,9,9,10,9,9,9,9,9,9,9,6,6,6,6,6,7,6,5,5,5,6,5,5,7,3,4,8,9,5,9,10,9,6,5,9,9,5,5,5,5,5,5,5,5,5,5,5,4,5,5,5,5,5,5,6,5,5,7,8,5,6,6,4,6,6,6,6,6,5,6,5,5,6,5,5,7,5,6,5,10,5,5,6,5,5,6,5,9,4,5,9,5,8,9,5,9,6,9,5,9,5,9,5,7,6,7,9,8,5,9,5,4,5,10,9,6,9,9,5,5,9,5,9,5,6,9,5,9,9,9,9,9,9,9,9,9,9,4,9,10,5,5,5,5,5,5,5,5,8,5,5,5,5,5,5,4,5,5,5,9,5,5,9,9,5,8,5,7,5,9]},{\"label\":\"max_leaves\",\"range\":[0,100],\"values\":[60,97,29,51,81,91,78,32,81,73,1,27,28,22,11,40,15,50,37,0,61,38,22,41,26,12,24,17,48,31,62,26,19,7,21,33,6,46,19,55,29,26,27,35,43,14,8,23,93,16,67,25,31,25,19,19,19,10,36,5,13,19,18,2,15,21,21,12,29,23,10,33,20,17,29,23,39,100,15,28,19,24,21,34,31,27,17,9,54,26,3,22,22,13,31,25,20,17,24,24,42,85,27,29,22,15,18,25,19,13,11,20,24,27,37,37,44,33,27,31,35,22,19,29,27,16,21,25,19,15,23,24,28,17,21,21,18,26,22,30,13,20,25,23,27,17,32,20,24,15,18,27,29,22,22,20,19,19,14,20,18,16,11,18,21,25,19,23,67,17,20,19,15,22,23,25,22,26,48,18,21,22,20,16,24,21,18,28,13,24,20,18,16,18,22,26,26,30,30,34,40,28,26,31,24,22,28,21,24,25,27,23,19,21,24,24,30,26,26,26,29,23,32,24,20,17,16,17,28,19,14,28,26,30,24,28,18,21,21,53,20,23,25,17,22,21,19,22,22,23,19,22,15,25,17,23,19,26,26,21,20,18,22,21,16,19,13,18,15,79,19,17,20,23,27,21,24,25,28,24,23,17,14,17,15,10,12,90,64,74,59,18,18,19,27,25,20,16,18,29,22,25,21,19,23,16,27,32,14,24,20,18,26,21,22,24,24,28,25,30,22,26,0,21,23,27,28,33,29,26,21,31,27,20,24,22,29,19,25,6,27,22,20,24,17,45,23,19,27,21,25,21,21,21,20,16,18,22,21,18,15,20,23,22,18,20,12,23,16,21,19,18,20,14,17,19,15,19,18,20,16,21,18,14,21,17,19,22,20,16,23,49,18,21,24,12,19,35,22,58,16,19,24,3,21,17,14,25,20,85,23,19,22,25,17,21,31,19,23,26,15,21,28,17,23,19,13,25,21,29,18,23,9,20,15,26,17,21,99,24,19,22,16,27,32,30,27,38,27,29,25,33,28,24,25,24,26,30,24,42,28,24,26,23,28,26,73,23,30,24,26,22,22,24,28,20,31,31,29,31,27,26,25,34,29]},{\"label\":\"min_child_weight\",\"range\":[1,5],\"values\":[1,5,4,3,2,2,5,2,1,4,4,1,4,3,4,1,3,3,2,5,4,2,1,2,1,1,1,1,2,3,1,1,1,1,1,5,2,4,2,1,3,1,1,1,2,1,1,2,5,1,3,1,1,1,1,4,4,4,5,4,4,1,1,2,4,1,3,1,2,1,1,1,1,1,1,4,1,5,1,1,2,1,1,1,1,4,1,1,1,2,1,1,1,1,1,3,4,5,3,3,3,3,3,3,3,3,3,4,3,3,4,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,3,5,4,3,4,1,1,4,4,4,4,4,4,3,1,1,4,1,3,1,4,5,1,2,4,4,4,4,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,3,4,3,4,3,3,3,3,3,3,3,3,3,3,3,4,3,4,3,4,3,4,4,3,4,4,4,4,4,4,3,4,3,3,3,4,4,4,4,4,3,3,4,4,4,4,4,4,4,3,4,3,4,3,3,2,3,4,3,4,3,4,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,5,2,3,3,3,3,3,3,3,2,3,3,3,5,3,3,3,3,3,3,3,3,3,3,3,3,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,3,3,3,3,3,3,3,3,3,3,3,2,3,3,3,3,3,5,3,3,3,2,3,3,3,3,3,3,4,3,3,4,3,3,2,4,3,4,5,5,5,3,5,5,5,5,5,5,5,5,5,5,5,5,3,5,1,4,1,1,1,1,1,1,1,1,4,1,1,1,1,4,1,4,1,4,4,4,1,4,1,4,4,1,4,4,4,1,4,5,1,4,4,5,1,4,4,1,5,4,3,3,4,3,1,5,3,4,3,4,3,1,3,3,4,3,3,1,5,3,3,4,4,2,5,1,3,4,3,3,3,3,3,3,3,2,3,3,3,3,3,4,3,3,3,2,3,3,2,5,3,3,3,3,3,3,5,3,3,3,2,3,5,3,3,3,3,3,3,3,3,3]},{\"label\":\"min_frequency\",\"range\":[0.00026768846242247214,0.9909933172419562],\"values\":[0.8661761457749352,0.18340450985343382,0.3663618432936917,0.17052412368729153,0.4401524937396013,0.5200680211778108,0.9218742350231168,0.3567533266935893,0.1987156815341724,0.11586905952512971,0.6668582750825782,0.021900671212648992,0.6526812002249811,0.691576321653761,0.6593158669766158,0.0032880947793703826,0.827787259457541,0.5039718682546217,0.7493133176953274,0.5906067965631687,0.33676317638312603,0.788407034545249,0.9734291410284917,0.7860640325023677,0.5849259177634188,0.5814242230842134,0.5847643039390392,0.27003445426356454,0.43292906250243435,0.05732103502828634,0.8993785404711319,0.6282611873114126,0.5634816745918383,0.7241414724190766,0.5335899229385099,0.41451718512581087,0.4885701019337033,0.2915475925822974,0.5409232380317441,0.4612472592597587,0.4014092516685638,0.6210976171590354,0.5651840863451802,0.7189275276084011,0.6295867487609423,0.683209275371736,0.5998498333710645,0.47426898975217124,0.5428454453741031,0.19803813522145108,0.8488782536064576,0.5655472247554886,0.6554304141369003,0.502124339443676,0.5672098232290248,0.7553489949323108,0.7407565414803378,0.7878210397270575,0.690386216877195,0.37104297682091236,0.11578780356480232,0.6531499437580595,0.6476375410692705,0.7086542699000127,0.604085491135479,0.8187345318320384,0.9041247100067288,0.8235357767197887,0.7697278118222646,0.8267097962143379,0.9672158672013991,0.6770111384363147,0.7647044346481557,0.8687756167777166,0.5556316464508324,0.5125766410086985,0.451505452316175,0.0032006194929241893,0.638408584012844,0.6093081170386995,0.244340786747557,0.5812750521211663,0.7081464154954805,0.5299267057128874,0.6567844258545373,0.5797833787007884,0.7427094933350528,0.8078466762148535,0.6183513352074607,0.8576891647527343,0.48269276728597743,0.814920008552173,0.9357865406850194,0.6751245595301829,0.5588078395090466,0.7995457996164979,0.7947065930907353,0.7375227641494841,0.13299985093788702,0.8922905592929775,0.765153125012589,0.04193884452572627,0.10291535495915634,0.12581721684614705,0.15708732704209669,0.10070733087610317,0.09074968263878193,0.8439615497838561,0.874922386812416,0.9335200999174125,0.8898822352812421,0.8021030596120454,0.8735842367849163,0.8222141714347942,0.8379778384818974,0.8310687248134376,0.8726556853082172,0.9112782478598584,0.9531694063154809,0.8739251846517917,0.7839366436521871,0.8044642097740621,0.8210505644382834,0.8466706650502209,0.7631734285118839,0.8802137712919232,0.7182402756915098,0.8387233864774228,0.8557649335469327,0.818390829523792,0.7826864338756854,0.591107851177905,0.6968050255386583,0.516595154850115,0.545725287620762,0.7552259909814745,0.9144220270340375,0.737686927360274,0.6353248128496429,0.4192478870692139,0.8130224423852682,0.5445718065931975,0.9909933172419562,0.579115787367136,0.5272830285815797,0.564197121705858,0.5973163539193012,0.5013398133406947,0.856150399093446,0.6592402487414981,0.7820387248748257,0.957651759975884,0.9413351803248012,0.6247024286446677,0.6430020723855022,0.6239967031880249,0.617075618064637,0.6207504506725486,0.6159699068494935,0.5961436187067249,0.6211206755300361,0.6671805178498115,0.6280032961924109,0.6148279108757388,0.5693853006796946,0.6020764466614219,0.5474958740343695,0.5477598619035415,0.6929014217804963,0.6393428147707482,0.6201163642466804,0.5553279134394924,0.5312484709760086,0.5766430307554337,0.5905655451436459,0.22995974014817755,0.5730529167006003,0.8343768495267616,0.6687441139440072,0.6088413797883153,0.8921001269050665,0.8730337581108222,0.8513492298734197,0.8978189626742649,0.8827895630513434,0.6396121531722399,0.5822872193528216,0.5429083612477433,0.8995154799253073,0.9171218004801861,0.4844413672705701,0.5638973615262038,0.5117202069509692,0.5582400147576669,0.5981822151858245,0.8321047032105265,0.8202410711040662,0.7973753376581519,0.7830417943672745,0.8001274210372701,0.295505040045096,0.839236320907621,0.8586211092782595,0.8011321255665627,0.8302353435364351,0.6254824328630414,0.8586000804423009,0.6523352457575908,0.8172086894622946,0.7704359993647825,0.800646138479888,0.824622363140925,0.8638772846519529,0.8417872224016819,0.821582701004345,0.8349359665330331,0.8125628690281436,0.7915953636080207,0.7872828616650891,0.7473465944020694,0.7698269508850503,0.8399361000035768,0.8008255397195497,0.8211830048263631,0.8472942427032949,0.8147661709686906,0.8098154245732508,0.8691524022454905,0.785691337134528,0.8235309738027684,0.7736231345755278,0.7908195810911853,0.8410087205569707,0.7962923845791351,0.7597326131369347,0.8108044736454963,0.7312073609505265,0.7836480251592646,0.8305106346801773,0.7814891934670146,0.7526816870025386,0.8082305952855824,0.7821485742126607,0.8535870294012741,0.815097135331865,0.8252879404283464,0.8783603247378533,0.8812716739939858,0.8914657458316695,0.8806435062455843,0.905299136781036,0.8865997947622977,0.9219608571296882,0.8701650572785335,0.8699474570918192,0.8935176231638611,0.7628755031261223,0.9252382802254359,0.9190019496755032,0.9099340771043969,0.9319358698766643,0.9552739766224394,0.9171649815668758,0.8992528028234839,0.9834419147420678,0.8892345260417713,0.8947089538882929,0.881029324226573,0.9424279779263806,0.9061186301866351,0.8651691795546609,0.8714043173580996,0.8496117255002276,0.9335127098823507,0.3435801933833305,0.9200157407624169,0.7178726323247432,0.728416575801094,0.6996792802880918,0.888042663160565,0.7076823891145565,0.7539869139601718,0.7506858894988027,0.7661213116947646,0.7231354718417738,0.6808148663777284,0.7401774685506508,0.7696623903574337,0.7855772095495586,0.39143022034773045,0.8110712928264565,0.8511552973782992,0.45682599939638524,0.292491572563281,0.05593627576517474,0.1712118374707604,0.8415429118664234,0.7493860325073761,0.7094920131415412,0.8465557038190705,0.25209347415413763,0.21051573355963843,0.8549587982266175,0.43797969057168706,0.6833217908664684,0.6518726900008242,0.789278749141507,0.8218426122670625,0.3162115613596685,0.860111460982912,0.7750669740217767,0.7379008769038715,0.8029217287966075,0.8316359857834804,0.4730092321473962,0.4767500049226519,0.4572695511749668,0.4680068065263623,0.488073464833293,0.4922378318716458,0.47747769071473567,0.45624146671507376,0.5180718796871933,0.8346400581153262,0.8252152041241914,0.5086183231385393,0.4838552869761553,0.5188666117464552,0.8337899899452698,0.43715659535872997,0.4125604167384248,0.4972019266363826,0.46846727879487005,0.9721228078038184,0.5274704127469162,0.45387156645398524,0.8546731838528547,0.5106391281064099,0.38307640320549297,0.028070632756788894,0.8173473780568808,0.9124799591458138,0.7973333368704744,0.9509532554729333,0.42692485514797607,0.5010563965740555,0.08835966951824437,0.8621099463126076,0.8071860296849284,0.5417114635214154,0.5477183513059637,0.5661849496791813,0.5357951942048624,0.5583981712073701,0.5656912814955274,0.5509091792450509,0.5328086675442513,0.5888670327142805,0.5651906889742001,0.5225475652256508,0.5776953508891056,0.5602938463915265,0.5871846324792547,0.5468273507558736,0.8795769485887535,0.9321953035920701,0.5420275575719902,0.5121581321542903,0.8469435811142318,0.9054329854881861,0.9221167761071452,0.9195534305324771,0.9053798003703204,0.934081149056889,0.9507835759282808,0.9002900609061456,0.8997296929787221,0.8845328046563802,0.9076686111834398,0.9294841023716408,0.9198986515908203,0.9653082146481659,0.8768981745376698,0.8906109332226648,0.9020313328938163,0.8653869967733864,0.9059336276230261,0.9414713525038063,0.8674399826138576,0.8803995654126326,0.6011887519213784,0.8297600802193384,0.8537730153342664,0.9230693365959454,0.5693929285538483,0.9529288883714048,0.8851297264049625,0.9026412705833057,0.8426585663734315,0.9831139980741709,0.8646028372499277,0.7939629087572224,0.15685768550532664,0.8142956646550077,0.9360073003332603,0.8371471737667862,0.8951947894245227,0.5788609580644798,0.9186396307286026,0.8701045344110977,0.7777269991634457,0.8182112258367231,0.272439790186505,0.8874178628669988,0.8536251412166459,0.5616698871774285,0.8041222161857434,0.9620049716800235,0.8423462801151241,0.7843753186733183,0.5991438648648812,0.9070644214073393,0.9381435673220154,0.8739248388493401,0.8250086034742622,0.5338421015419239,0.19911172819705092,0.9160158306435371,0.8556050048963303,0.8903596307744416,0.7699935512018251,0.5557939617145723,0.7991416625002637,0.8455260924876585,0.5862527935202576,0.9277560779237538,0.867677569606165,0.8202762381790274,0.9091607436350305,0.3656580636572389,0.8363238692106675,0.8828317346673419,0.8031085144925816,0.003273485431639467,0.019037843123732738,0.09154689588586427,0.07191141949983477,0.5693541947548199,0.04180981061988758,0.5370815085745426,0.1106384398354035,0.7771997779900248,0.6098102694550175,0.01936985139723842,0.13604016123771087,0.013852927524222477,0.045823800187426,0.010840511769323349,0.8577344775002347,0.02634336703539797,0.056128559744525366,0.0073960259755175665,0.040194140255798795,0.034054160533304165,0.0029372346639876153,0.06357554022384154,0.00026768846242247214,0.8297063482835972,0.023088332574004437,0.5503897150132052,0.02038519741584913,0.013863151928434266,0.0005790081298963677,0.07623920403051909,0.035648245624524864,0.030509163920078138,0.5824871442081219,0.052337432215525656,0.575857873708026,0.8698116957995166,0.9461913090364594,0.016269366798427415,0.598118454044031,0.8346922499160262,0.8508514307627728]},{\"label\":\"n_estimators\",\"range\":[55,989],\"values\":[954,723,549,796,952,520,225,236,566,825,55,528,497,392,385,669,421,258,644,467,324,657,599,743,476,489,335,87,311,145,318,447,369,384,189,560,345,512,605,915,290,439,520,357,417,484,597,264,196,554,360,519,696,402,456,428,433,367,459,267,403,485,508,581,545,472,630,467,297,341,491,386,434,548,489,410,448,329,526,579,461,481,375,419,505,545,632,473,437,388,535,361,341,455,498,311,351,298,318,212,233,314,275,156,276,248,284,324,366,396,366,331,298,254,300,180,230,263,255,296,208,423,376,280,305,408,354,778,448,377,345,312,252,284,425,426,469,400,362,442,512,342,970,328,390,417,489,301,367,461,243,262,276,318,324,342,300,295,300,289,317,316,881,272,225,339,310,292,309,337,241,268,319,360,352,377,289,327,303,259,354,351,335,391,358,316,283,376,308,404,347,433,431,282,330,368,411,381,384,436,369,395,351,412,383,370,338,452,300,295,266,326,474,355,304,304,247,320,278,299,330,311,323,356,285,319,341,303,275,263,282,217,362,247,296,327,311,265,274,232,275,60,340,262,300,295,312,284,260,254,230,272,249,288,496,320,271,314,314,281,260,200,282,242,296,263,263,234,524,272,249,286,290,317,336,224,259,250,217,270,476,253,233,284,187,300,257,278,500,313,240,459,446,471,435,444,471,210,457,488,445,508,269,251,428,468,457,288,416,296,263,278,239,305,303,307,303,486,300,312,452,420,522,282,330,305,308,332,296,403,334,317,437,289,927,301,484,273,718,317,344,283,123,448,547,509,838,304,464,328,330,347,346,328,330,358,321,345,323,272,388,293,336,316,369,483,428,305,271,403,398,411,391,424,407,384,406,385,400,374,427,390,449,363,418,444,464,381,401,372,432,356,396,417,460,670,438,356,221,345,484,382,499,243,410,262,442,370,342,282,458,532,400,473,249,425,346,326,287,381,264,331,362,285,502,239,206,435,395,417,314,267,453,349,298,326,230,989,373,474,255,280,410,305,304,319,297,273,573,336,288,171,317,257,231,255,251,202,253,268,219,236,274,252,288,267,295,231,306,261,280,336,318,289,241,216,356,354,252,350,272,307,281,369,515]},{\"label\":\"reg_alpha\",\"range\":[0.0026679612772813055,0.9999611589026972],\"values\":[0.15599452033620265,0.21233911067827616,0.13949386065204183,0.046450412719997725,0.09767211400638387,0.662522284353982,0.8948273504276488,0.2713490317738959,0.9868869366005173,0.07404465173409036,0.44307979680070225,0.9024870701034325,0.6895366011206419,0.7347062987020356,0.7473828198583863,0.5085491925453045,0.8504047001387413,0.5657461512443864,0.8734496043405008,0.34910396732929977,0.6569388172687955,0.8527589962213578,0.7929947995359711,0.9696048638251101,0.6381436904107733,0.6340129925737212,0.5783806248803782,0.420315664402689,0.5575176515728074,0.7280253875666061,0.9272191689294649,0.5999307018729003,0.5076497231093915,0.4849167644771975,0.3684333565436956,0.7959478041823813,0.6970945481847112,0.5276171558859993,0.0026679612772813055,0.3334978526479885,0.23787931010578517,0.5896462276910432,0.6337032563293991,0.44201324313504986,0.7924388763283952,0.6806664539457935,0.47885363648877993,0.5465384209034652,0.3922447741327396,0.16342952420781776,0.6174565089485323,0.6448089121902647,0.7061543439753944,0.5778346277231564,0.7643808392045331,0.7548051858166924,0.8299034276311184,0.9300523911630576,0.7552103612655474,0.7672336623810965,0.9988559932601531,0.8925833822850073,0.8722207190393543,0.9194394032104186,0.8250771360671719,0.9589084141547891,0.9684661359300096,0.8886687419675168,0.9453044230484035,0.7247101746339885,0.6707099323087259,0.9041962378213809,0.838378548491587,0.95611851371659,0.7756290123043967,0.8077973001213081,0.8559885672419014,0.8891665339911246,0.5178439031523833,0.704571980823008,0.6067333451373671,0.5414494626192138,0.49764089051518956,0.6624192863622046,0.7413251382439893,0.6237014656353669,0.4697374165861461,0.568586358719845,0.6886569527042427,0.8126337177663986,0.9841690739699394,0.7348948261121444,0.7128142775143664,0.6410515994212458,0.7805420626610617,0.7464808225546941,0.7490375167927078,0.8630866168150952,0.7589234864122312,0.7584847094810766,0.7263235774030263,0.7887607647995489,0.9097810348761016,0.9067961061630769,0.6845803096786834,0.9351695924214545,0.8431182216673385,0.8228331942686823,0.878740276790875,0.8737310135971577,0.9524785358985902,0.8065718374946022,0.7681208155612336,0.891587464952773,0.9073053790644477,0.9119117565835736,0.882876300815881,0.3115196095384465,0.9825595302275982,0.9277901986628261,0.8446018827632942,0.9674932174245477,0.8616315985270148,0.8995617835947204,0.7330334786917263,0.8832762484041279,0.7755756539268259,0.799393460687683,0.9460165941794653,0.8291217561300994,0.9176743074277134,0.7091592384927234,0.43621817568857557,0.6634803032817803,0.998202678618052,0.9661165493812959,0.9985353094298778,0.9267350208262465,0.9475378223452345,0.895142845372112,0.9760135398009898,0.7259801825342462,0.4022932212036484,0.7467732220317719,0.6925271700334762,0.5871978844318791,0.5292482903713306,0.46520980098897474,0.8605558916221192,0.7945711428347217,0.7670536638301252,0.9893101359385794,0.9744468186401201,0.9381059189777615,0.9207776241885709,0.9401397690535555,0.9418430143097762,0.946492604459071,0.9390691159656532,0.9594144599337724,0.9356876655576489,0.9103296065597996,0.941916225517524,0.943774875454871,0.9597886325371423,0.9064405185277192,0.9956443122188541,0.9767311302771365,0.9987517320239839,0.9286979333954314,0.958068047833454,0.8807059968880715,0.9822478032926323,0.9303770515510267,0.9403801935540317,0.9148848670011146,0.9604474129795539,0.8924225068205675,0.9983358793995329,0.9232382771242009,0.9352632209542496,0.9343777204263812,0.903950744351043,0.9610592141954732,0.867724978190005,0.9730390957650534,0.9460177384730728,0.8898551228434274,0.9221197913221962,0.08571749663978478,0.8484537989081812,0.9792066755751656,0.9838080986640607,0.9597552110512566,0.9338517090546014,0.9090929274331019,0.9082718494743512,0.8831929043806223,0.872675154393864,0.8882239290043052,0.9038701416219665,0.9270640037543538,0.9492406471003003,0.9154371383415406,0.8726158644320051,0.9376425526070035,0.9706401217536198,0.15856619209245376,0.9044294657321159,0.8932581329510167,0.8356041863883118,0.926884917017842,0.9070411320749024,0.9455104338263094,0.9523008148818036,0.8578977931531181,0.9542584388708849,0.8847328735122444,0.8925847715300098,0.8767117799307781,0.8177877681852872,0.9182954750625163,0.907540198485619,0.9479420435307302,0.9227366055958461,0.9472322661357085,0.28263670702209537,0.9685042275991287,0.8858750354036293,0.852810640841728,0.9552359085292325,0.8841651087863633,0.8972154377478904,0.9159304715820411,0.8672301884484277,0.9401132783689631,0.893048414533083,0.9224849208166508,0.9353360413606082,0.9725966147562012,0.9114161144454744,0.8776767632982694,0.9217308362393837,0.9532081963283504,0.9090691170724441,0.9314520255501716,0.951142536228952,0.9053980897461079,0.3604626502811149,0.9094172763783294,0.12516912303059913,0.27628913131604255,0.7475165592758766,0.22579808726659178,0.33616452358374516,0.7769896859695284,0.37621731537713987,0.4943389454034048,0.5575636962142967,0.44225019497842216,0.41501584877310693,0.44599316030153113,0.38083442190783257,0.48826550458014273,0.29599928875811155,0.45848645400297233,0.47907554897014015,0.46499760347755337,0.514038193818931,0.43903672621527806,0.49666813670534243,0.5305870581949793,0.3468738045325548,0.4528802278819297,0.7201035079609222,0.4201485862821145,0.39590178903397527,0.42941501981641006,0.353178238550576,0.46632917418448033,0.3601161141477385,0.41269267310366553,0.41656790317895454,0.4573316064840945,0.45493130271236254,0.3878384043547179,0.4215113098234823,0.40137367555126147,0.487929788533304,0.40327943938373473,0.008844259347880135,0.9773779197457364,0.9809896077368842,0.9768138511489597,0.9873386490162266,0.9820843176439278,0.9643733518812603,0.43551131375525415,0.9769227019151692,0.49984023002409667,0.18769230929824032,0.655149342880547,0.9970402216419356,0.67894687450282,0.9653151250980283,0.9575931785920347,0.7635522495478038,0.4745610300032459,0.6225325872163086,0.698398015557118,0.3883534215386581,0.9624887744974491,0.51514153342088,0.7865377920934118,0.7922421958505755,0.793284913026125,0.7991513240278871,0.7808195533231357,0.8026529060280204,0.7487658940356815,0.7880809077611778,0.8165469897836628,0.7670683159414309,0.730354505543044,0.8277994907481234,0.7846371226397492,0.7792905742735063,0.7535343569485417,0.8046023456609352,0.7890423705028066,0.8408669296294269,0.7354692820966747,0.7665199738328007,0.7947030528973327,0.745050467382743,0.8210501320614059,0.755239216461376,0.791036352563135,0.7736528548999795,0.810716338947436,0.7169062130693008,0.9982108118739812,0.8407232357777287,0.9846554156461076,0.7608391419062938,0.954517795918556,0.7775740192861067,0.9235230979250315,0.9731225005682691,0.8594105683144001,0.8612892671261827,0.8308107779351356,0.8257051268795352,0.8510076691910381,0.8202117034460961,0.8622941805984828,0.8423704775627892,0.849740892548467,0.855092411985291,0.8267630832164481,0.8058476460337363,0.8742357614435776,0.833910933919677,0.8727302560099031,0.8482190610218422,0.47851342615300546,0.8122965948955914,0.5057307342203016,0.5423793510679461,0.8908953200127485,0.891255359997654,0.8900209099548235,0.8726405907054832,0.6009650425325579,0.8962368333418014,0.8910167310679538,0.8905879199579895,0.860042155209858,0.8982436894735166,0.877569997704308,0.9187685791347249,0.8972437287246058,0.8584399891038572,0.9227273438049283,0.9397913875077878,0.8829233464960322,0.864783893459356,0.9122717554259534,0.9644214335385792,0.9358545226463663,0.9049276538093777,0.4510443648225074,0.842015200860315,0.2560973973317221,0.9806197908658266,0.8840590029466021,0.9576603067771373,0.921393576031389,0.9974521716735592,0.874185544248192,0.9456128949831308,0.8535911007648318,0.8980259496731569,0.907058961751213,0.9727827208497298,0.9293414009324114,0.49027540199790887,0.949118408324486,0.44213606870656047,0.8753660018875956,0.886358396952139,0.8438263545479552,0.9223510351311214,0.47241025709029333,0.9823115998557382,0.9586461814989129,0.8577913837700829,0.9030930882733477,0.5288737176038959,0.9999611589026972,0.934621179909776,0.8372876178500536,0.8897784365603835,0.5720302876636748,0.9153774562680075,0.8662255658770722,0.9730832251727243,0.9408043295999344,0.8304109950061765,0.9571998494885275,0.9187120338920304,0.8933832358905379,0.5041624079697357,0.4630150140560788,0.8817423311216902,0.8595592653857486,0.9806845290078774,0.9340511787511604,0.9020564857562541,0.7076846045444984,0.43612727063247597,0.9449181106690548,0.9616499288172236,0.9150836728224352,0.915655268668594,0.9245262021822328,0.9739107303577528,0.9991948571732886,0.9494489718097806,0.8202896159829404,0.9334687301015878,0.9080930730632161,0.6708942773558241,0.864383952855516,0.8937324445152133,0.6360520253805666,0.6394017674739539,0.863874269744447,0.547507800787344,0.5912546961709388,0.6837367690479945,0.870216117289339,0.8427274303999306,0.6140016718048749,0.48536109001114097,0.8510340152898953,0.633337450317703,0.867270209415818,0.5657057415207979,0.517172050988739,0.44970185195896717,0.8822247468252178,0.8374893734612561,0.6652242687490935,0.9091816464234166,0.8810317664214504,0.9214865304958775,0.9243403016534819,0.9047231926007392,0.9490947648624299,0.9298083897974874,0.9104706921097664,0.9316869045511197,0.8937100064838698,0.466580738579494]},{\"label\":\"reg_lambda\",\"range\":[0.013179214380923399,0.9881136084991532],\"values\":[0.05808361216819946,0.18182496720710062,0.29214464853521815,0.6075448519014384,0.6842330265121569,0.31171107608941095,0.5978999788110851,0.8287375091519293,0.7722447692966574,0.3584657285442726,0.9597707459454197,0.38396956718466113,0.41655711627713987,0.4381301674911343,0.4621147584011279,0.1639473301667732,0.5278038717786437,0.013179214380923399,0.2511546236069977,0.3803227602171291,0.5335413017423069,0.23969505489801748,0.20259695317130366,0.08796388409564204,0.3740450374817565,0.4000251578372295,0.4854106304299757,0.6598858258787057,0.4772837469516771,0.5377387196570456,0.12347883698539719,0.3059764342203778,0.4095058575647049,0.4442468302752816,0.3378638945366279,0.5977468166263169,0.4086224846878719,0.27224006825843433,0.50678056831219,0.6721195355068721,0.5814079897636355,0.32421237438028955,0.3661680130107681,0.41645466029171546,0.4630440329768555,0.5655166465587148,0.4894969142124765,0.766167925264151,0.2077641458314251,0.6336438389068342,0.3532703273165313,0.36912227021291555,0.2902653144828959,0.42996972697379254,0.39213612295472194,0.3944761779335692,0.4995539628364495,0.9881136084991532,0.40092860356420407,0.5584397474449574,0.45809613703873087,0.329708433760996,0.24402833150696762,0.32810187669598245,0.2767639435316313,0.38790704745858584,0.44639463984831446,0.509507189677391,0.31286204395662665,0.3887760702660074,0.1621733489987021,0.42644615857964174,0.3493138926612585,0.39150931173129766,0.3433113440092751,0.46868140659625096,0.5342648895595876,0.42196164566569017,0.3080167238778123,0.9097597741269523,0.2658226527843706,0.36307431223359027,0.48339511186447626,0.4026081188326501,0.3693412826727845,0.4322429053621167,0.38641165242443903,0.21401363972417853,0.2903946728618218,0.33377486319835786,0.4477147087813658,0.38404606401630803,0.413844598048159,0.3736668206075911,0.5135195661219588,0.4809384392728861,0.47840472463704253,0.4523991394021183,0.5473265650531962,0.5579789479833013,0.5851687726534637,0.6099483960644579,0.40878659481606927,0.4947809941268915,0.5284463360240309,0.41424337811776896,0.4569302432182551,0.4366782957609293,0.3467155109095136,0.35244628445616627,0.31998584168711114,0.3939694898296057,0.335066136960884,0.33735306944949706,0.29979447635281936,0.29679064964646396,0.23389341975271172,0.2630390820948905,0.33237753756730004,0.3493049119991466,0.2864025046174937,0.38184651992190377,0.3049413857560959,0.40583730258026823,0.32482962578522484,0.3563976737529813,0.37470582912316686,0.4184934993847478,0.34065299815674105,0.3961932606647727,0.3674698955154728,0.4728809997757596,0.4390595488563487,0.3809706785200698,0.3185211973347811,0.3160292808529903,0.2624081233516019,0.28567162828860504,0.2260990290808247,0.3386673816550568,0.35425148757367675,0.4266569327675732,0.3998089219228228,0.3239501637355046,0.30681028447329706,0.3680665883923674,0.4639021919535816,0.42026022839693294,0.39074796579159077,0.4501510551286619,0.5106343656192395,0.32537768472963613,0.3390696944397587,0.36386297155016234,0.35821597769619234,0.4045074888054844,0.3810168137987222,0.4037105649214041,0.40153772200782056,0.42734094146561286,0.3849922240058435,0.38016223071140853,0.4046935684596782,0.3659458970841972,0.4074614390912723,0.43471861499495973,0.3509204848496648,0.356611562684226,0.3020396892285595,0.34547888944116667,0.04681423217522829,0.38620980649912545,0.3747845477744996,0.40366425420540986,0.4123726923385627,0.4439973255751393,0.34550315677535787,0.2803059658077491,0.3137716560908162,0.36631565937369864,0.39138280165287487,0.39446249210213347,0.4248495214730885,0.38380116467856734,0.41100900659378453,0.36182145068623195,0.3312957594296668,0.46827929843548965,0.3762684924381597,0.4432196040446659,0.3499951547723043,0.39740908911190675,0.4046478679083891,0.4208746196083659,0.7375661664659416,0.3925675191938664,0.3903463213007663,0.4348570510350322,0.4539414435123373,0.4316174757550938,0.4167660026446131,0.3950525416062296,0.4867870398120812,0.3797864055019265,0.4095749518381335,0.4337919026906944,0.3710842730297647,0.399928342059878,0.4502850070562409,0.46734009383545166,0.45020957419847835,0.4237121910117033,0.37317853022144476,0.3947663554338227,0.3327076224627447,0.8605259249661349,0.4423842755731467,0.40816388288208294,0.41754255405027774,0.48490078017961646,0.40315921207392,0.3846442969960016,0.3380435454384824,0.36416732976316535,0.43291744088507284,0.40830061032457576,0.41165132697898393,0.4659659483451544,0.3964748679249158,0.38634094515604733,0.4153086422555161,0.39881032323675003,0.43743977859621036,0.3853638006418121,0.41096878196076425,0.42786665028614373,0.34625304464819867,0.4520439786927109,0.45778093889873467,0.37097964322268423,0.3925307490404483,0.44354688980142143,0.4241169004463233,0.40486461986002514,0.4522333916669145,0.4889619300595772,0.4567594398363724,0.459728685429116,0.473303904725486,0.5091880373638954,0.4821375959779609,0.46130932698990523,0.4717774963152931,0.4562711780391801,0.5061116556829203,0.49404885099847196,0.4627415228893643,0.45258064870632064,0.4384909204278187,0.4762871912209316,0.5127295401072753,0.48158070165373473,0.535971677016202,0.4778923768823633,0.4510159696669657,0.42669295918764727,0.4951774846682421,0.4666305817919228,0.43695691682386084,0.4277141467560722,0.45213577573609054,0.47255569888579835,0.4244722719559093,0.44480525672455784,0.5231778384634093,0.3578363394229679,0.41662267589994156,0.4879899763794195,0.42399889130569185,0.46144202855461136,0.44761950682295254,0.3295061025305066,0.32144535770827026,0.332576307922019,0.3464508866536366,0.3736397938264038,0.30187332244907694,0.41006724641021314,0.3578705311014547,0.3859054600584723,0.32839208160314115,0.41929433384100334,0.4268873647070798,0.427502637455376,0.437653270482337,0.4208173501143711,0.45040516803972175,0.3953090452115776,0.4208921933832745,0.4418078365353464,0.38294655061209465,0.41431512622601985,0.4985443935272281,0.46509794434242996,0.3975113632571862,0.43458273416837756,0.37211662793032646,0.41688850423948426,0.46393387874341,0.39413904702504,0.44250306059913347,0.47712901678468866,0.3613440019058738,0.4098267253015448,0.4151358700226637,0.4136558638949496,0.415931174641875,0.40588376253611064,0.42766267417222753,0.39990510795427714,0.43378645048550296,0.4087400147905977,0.44784230165198724,0.3802957320980286,0.4386807381416588,0.41074199231790837,0.41898365558570155,0.39577696340906476,0.3853710737111703,0.4567210194029208,0.42575368911640005,0.3710910642998164,0.39706132351171836,0.4425805244184377,0.49307132938830894,0.42356690276297243,0.47495617347173313,0.4042505979969816,0.7111973056839338,0.3626858081572065,0.4516198005524019,0.37697508039508193,0.13760618537715597,0.4310890212685601,0.4045517380526882,0.3869352641956634,0.46132845992180826,0.4119552265508287,0.35598693654598623,0.5116396512252568,0.5172045105263017,0.51750575369899,0.5562669225308655,0.5390959955422605,0.5738901112486108,0.5318102025429393,0.5467753160207335,0.5258174812835823,0.5010900554398923,0.5321089700384776,0.5195441610502476,0.49624988819784444,0.5415320296150832,0.6133194408935142,0.507640362941831,0.475887993404029,0.5863695635192681,0.4674730667991611,0.4863090914941466,0.5582452175856026,0.5720183223417946,0.5904979122352721,0.6101421366216984,0.5579602428610132,0.5485119825987282,0.5782287097284724,0.5858853412727129,0.6329294551745438,0.563619140700045,0.5792789231147802,0.5720259876880028,0.5616005706181868,0.5339769414072832,0.5934801710303227,0.5474235463424112,0.6268114217606866,0.5164662788317058,0.5667322792368104,0.5178023179504041,0.6062002056940502,0.5688522118442273,0.5330188396865414,0.9071560607401502,0.5341734321527065,0.2834039104309857,0.5014344479615604,0.6604604860433858,0.5586271546101723,0.5448482004119863,0.6014025315241252,0.5073024389817505,0.3125921503779366,0.48375896229170406,0.4425547161755717,0.5801786637869668,0.3483309440569761,0.44632793881916333,0.8010123722207089,0.4831619393428359,0.5510113221140125,0.45338309708092506,0.5127240607433814,0.3397750010155828,0.29476210592053514,0.46846430175646114,0.49060411572098384,0.37476791084626415,0.4297571263280544,0.3151948128726586,0.45720281818292846,0.5307795063890117,0.5693218868672512,0.43537222669975917,0.36653445310314736,0.3911739559282172,0.429043490368923,0.5074514712055922,0.2646105576584573,0.47256138405542214,0.342050239107774,0.45045618067480386,0.541703067782517,0.6016339011429416,0.38910417758575677,0.43213186104870777,0.4882403685577428,0.08240332140497997,0.41063219234339465,0.36182636521101075,0.521384829991404,0.45670264778334296,0.5889169956763148,0.42620072895281524,0.32382988072744884,0.3054755444131598,0.32337751353756045,0.3261364033758126,0.34069867908806245,0.28799130475878815,0.35288716163568623,0.3832732928208923,0.24043117485976884,0.3135093512315231,0.3766438337684536,0.3560198623152882,0.37279528693110087,0.37100330567322504,0.33074371093284716,0.34367938490284594,0.37188538499711155,0.353417908581131,0.3830971043644229,0.33032160740057726,0.3949901192664312,0.3043332962952778,0.37343895175557534,0.40132431363791343,0.3495872209177999,0.36217247865050584,0.39753716539177303,0.028529588311438592,0.4949155400322512,0.47036735494663906,0.32900860897802275,0.3776728153891716,0.4106186506443235,0.4393805442095564,0.4441521022317597,0.45130745751068024,0.4173471687680255,0.46965822499892795,0.6865452278671351,0.43874020950553544,0.4378523401795997,0.977189045808053]}],\"labelangle\":30,\"labelside\":\"bottom\",\"line\":{\"color\":[7007.95703125,6637.349609375,6610.51220703125,6656.84765625,6863.22119140625,7395.537109375,6867.76806640625,8673.1630859375,6613.5908203125,8707.541015625,13298.498046875,6590.04833984375,6548.84375,6626.29833984375,6914.8203125,6751.91259765625,6829.97900390625,6625.62109375,6604.37939453125,6727.705078125,6605.10595703125,6594.95703125,6600.333984375,6638.56494140625,6590.1220703125,6650.185546875,6583.19384765625,7634.36669921875,6667.81884765625,7166.736328125,6686.90478515625,6608.29833984375,6559.06494140625,6789.44189453125,6679.92333984375,6641.423828125,6718.26171875,6787.703125,6616.7685546875,6703.5595703125,7266.310546875,6595.31982421875,6594.42431640625,6622.9482421875,6650.20947265625,7067.29296875,6749.18310546875,8432.1220703125,7053.7265625,6619.82421875,6785.34521484375,6593.25341796875,6645.6728515625,6641.09033203125,6579.16455078125,6563.669921875,6623.27685546875,6682.27490234375,6651.50390625,7146.3701171875,6644.376953125,6580.2744140625,6604.23583984375,8375.7509765625,6607.9248046875,6558.736328125,6635.4072265625,6631.64794921875,6640.908203125,6590.23583984375,6603.72216796875,6627.794921875,6597.07568359375,6595.22314453125,6591.6123046875,6609.59619140625,6776.685546875,6601.31982421875,6615.09326171875,6602.89990234375,6746.00390625,6679.05517578125,6666.22607421875,6629.84423828125,6599.34521484375,6660.59228515625,6618.35546875,6706.54541015625,6655.7158203125,6591.0263671875,7853.14697265625,6582.80126953125,6601.70068359375,6605.2529296875,6683.19384765625,6565.310546875,6610.626953125,6616.51171875,6584.5380859375,6610.31396484375,6644.37841796875,6680.05029296875,6543.23828125,6719.845703125,6587.00927734375,6701.169921875,6609.30322265625,6608.14404296875,6572.1025390625,6636.12890625,6619.3857421875,6588.419921875,6556.26611328125,6528.5888671875,6575.0986328125,6665.62158203125,6702.54443359375,6715.271484375,6584.46875,6585.62841796875,6631.24267578125,6629.72119140625,6607.30712890625,6590.86767578125,6625.8095703125,6600.49169921875,6595.5185546875,6710.02392578125,6592.51220703125,6624.1279296875,6609.70361328125,6610.90185546875,6625.0068359375,6749.16455078125,6577.2158203125,6590.9765625,6627.12353515625,6613.42431640625,6600.28173828125,6609.42431640625,6655.67919921875,6632.33203125,6809.95458984375,6593.06640625,6599.29931640625,6590.09130859375,6679.12548828125,6666.6103515625,6588.82275390625,6609.87646484375,6660.3017578125,6587.96630859375,6614.4931640625,6576.681640625,6625.94970703125,6572.5146484375,6575.1962890625,6568.24462890625,6593.33935546875,6607.05419921875,6568.814453125,6596.18505859375,6631.07958984375,6597.71484375,6615.2060546875,6607.01953125,6578.537109375,6593.20947265625,6829.83154296875,6590.26025390625,6604.158203125,6589.60009765625,6614.46826171875,6573.06982421875,6612.64208984375,6624.3447265625,6591.3388671875,6589.263671875,6661.00927734375,6630.7216796875,6578.18896484375,6593.51171875,6616.89208984375,6597.83349609375,6609.42236328125,6589.16845703125,6597.66455078125,6637.5048828125,6686.10400390625,6614.4375,6588.76611328125,6561.1044921875,6600.64501953125,6609.5830078125,6593.62060546875,6567.78955078125,6594.572265625,6576.23388671875,6606.31201171875,6635.32275390625,6614.4814453125,6588.06982421875,6621.392578125,6606.3701171875,6593.89794921875,6592.52783203125,6591.3642578125,6620.32080078125,6574.45556640625,6578.5966796875,6581.85107421875,6582.421875,6599.54345703125,6574.74365234375,6532.60009765625,6596.35546875,6583.041015625,6573.2919921875,6597.62060546875,6613.6943359375,6582.4345703125,6594.70703125,6600.59912109375,6590.03955078125,6591.93115234375,6571.4482421875,6612.27294921875,6587.837890625,6550.25732421875,6578.9228515625,6622.44677734375,6609.84033203125,6598.27783203125,6625.7021484375,6591.974609375,6609.1005859375,6576.310546875,6537.71484375,6600.22607421875,6590.798828125,6609.63330078125,7805.06298828125,6583.7060546875,6671.68603515625,6543.3603515625,6577.779296875,6558.150390625,6569.7392578125,6572.38623046875,6593.24462890625,6586.2919921875,6581.19189453125,6638.31005859375,6784.13818359375,6605.94384765625,6592.0595703125,6606.14599609375,6554.6279296875,6582.47216796875,6536.71240234375,6591.830078125,6577.14306640625,6576.5849609375,6592.8515625,6620.37548828125,6562.462890625,6641.93408203125,6591.19384765625,6631.07177734375,6853.306640625,6576.81103515625,6580.25,6602.25,6584.150390625,6691.3173828125,6593.04296875,6568.3662109375,10161.9873046875,6587.533203125,6621.29052734375,6749.8271484375,6571.333984375,6666.78857421875,6604.47802734375,6733.06787109375,6662.67138671875,6797.69287109375,6904.7841796875,6888.63427734375,6591.7197265625,6704.88134765625,6556.552734375,6563.73974609375,6601.4072265625,6595.5009765625,6597.5517578125,6629.384765625,6715.6103515625,6588.869140625,6635.31591796875,6597.98779296875,6642.17822265625,6571.560546875,6588.05078125,6608.162109375,6612.2158203125,6675.58203125,6634.9697265625,6676.99951171875,6603.1181640625,6642.89111328125,6590.1259765625,6585.48486328125,6554.6171875,6564.53515625,6566.59033203125,6584.2841796875,6644.0390625,6585.470703125,6604.2255859375,6620.7236328125,6621.93408203125,7560.08984375,6579.1162109375,6577.96826171875,6566.1396484375,6596.28173828125,6587.529296875,6583.46337890625,6586.00927734375,6579.845703125,6597.04638671875,6572.91796875,6577.1904296875,6785.4033203125,6591.53173828125,6666.28369140625,6603.326171875,6711.982421875,8425.92578125,6803.56884765625,6568.7509765625,7198.23193359375,6625.451171875,6645.29052734375,6731.42724609375,6706.224609375,6576.3369140625,6625.2783203125,6557.51318359375,6589.97314453125,6545.830078125,6667.28515625,6563.29052734375,6592.4697265625,6600.72314453125,6591.86328125,6580.67626953125,6570.4833984375,6661.15966796875,6620.35791015625,6573.8583984375,6619.08251953125,6570.7041015625,6592.2978515625,6607.00341796875,6656.5634765625,6580.39794921875,6605.419921875,6563.88134765625,6546.80224609375,6564.86376953125,6572.33056640625,6659.5078125,6583.82958984375,6553.6767578125,6629.984375,6677.3466796875,6569.54638671875,6576.87744140625,6595.6318359375,6602.974609375,6583.3896484375,6618.5927734375,6590.24609375,6585.7158203125,6608.4306640625,6577.91748046875,6600.65673828125,6590.71240234375,6593.2099609375,6774.93896484375,6583.18017578125,6581.68994140625,6668.25146484375,6670.81982421875,6605.7744140625,6744.1220703125,6786.28955078125,6578.77001953125,6615.0234375,6598.29296875,6620.68896484375,7956.79296875,6570.2587890625,6626.36474609375,6591.9150390625,6597.001953125,6567.3291015625,6619.43994140625,6622.63427734375,6618.7841796875,6644.69091796875,6646.8974609375,6709.55126953125,6612.52099609375,6618.53271484375,6569.0732421875,6634.6982421875,6611.7265625,6691.50927734375,6585.9482421875,6655.29052734375,6621.0400390625,6586.20654296875,6646.63037109375,6795.59619140625,6610.8505859375,6582.2255859375,6606.8408203125,6630.30908203125,6603.73388671875,6623.03564453125,6602.6357421875,6641.45361328125,6622.33935546875,6626.79541015625,6691.10107421875,6582.72119140625,6662.9873046875,6641.14404296875,6573.1650390625,6591.7373046875,6529.39794921875,6644.50048828125,6584.35546875,6587.0166015625,6633.24755859375,6704.99755859375,6617.91015625,6588.4921875,6660.03466796875,6621.1162109375,6548.83544921875,6565.10400390625,6558.2626953125,6573.3251953125,6587.18310546875,6571.353515625,6587.64208984375,6605.22265625,6599.490234375,6596.361328125,6590.0400390625,6584.62255859375,6574.0068359375,6612.748046875,6604.1650390625,6595.9501953125,6611.99755859375,6625.53662109375,6587.25439453125,6575.4794921875,6658.87451171875,6718.22998046875,6632.5498046875,6562.4833984375,6619.6513671875,6589.77587890625,6645.5546875,6583.0517578125,6588.24462890625,6582.66845703125,6679.640625,6680.68798828125],\"colorbar\":{\"title\":{\"text\":\"Objective Value\"}},\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]],\"reversescale\":true,\"showscale\":true},\"type\":\"parcoords\"}],                        {\"title\":{\"text\":\"Parallel Coordinate Plot\"},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('ef44c656-4a9b-4fbe-9f55-8b3d54d25ba2');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 2. Gr√°fico de coordenadas paralelas\n",
        "fig1 = optuna.visualization.plot_parallel_coordinate(study1)\n",
        "fig1.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"575063b3-4301-430f-90ac-f1fd74618bce\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"575063b3-4301-430f-90ac-f1fd74618bce\")) {                    Plotly.newPlot(                        \"575063b3-4301-430f-90ac-f1fd74618bce\",                        [{\"cliponaxis\":false,\"hovertemplate\":[\"min_child_weight (IntDistribution): 0.0004122194057874021\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"max_depth (IntDistribution): 0.0031878440376277304\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"min_frequency (FloatDistribution): 0.0041492086714428845\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"reg_alpha (FloatDistribution): 0.007321534468440911\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"max_leaves (IntDistribution): 0.061823643307925086\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"reg_lambda (FloatDistribution): 0.07425627890568393\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"n_estimators (IntDistribution): 0.3363717392722931\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"learning_rate (FloatDistribution): 0.5124775319307989\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"],\"name\":\"Objective Value\",\"orientation\":\"h\",\"text\":[\"\\u003c0.01\",\"\\u003c0.01\",\"\\u003c0.01\",\"\\u003c0.01\",\"0.06\",\"0.07\",\"0.34\",\"0.51\"],\"textposition\":\"outside\",\"x\":[0.0004122194057874021,0.0031878440376277304,0.0041492086714428845,0.007321534468440911,0.061823643307925086,0.07425627890568393,0.3363717392722931,0.5124775319307989],\"y\":[\"min_child_weight\",\"max_depth\",\"min_frequency\",\"reg_alpha\",\"max_leaves\",\"reg_lambda\",\"n_estimators\",\"learning_rate\"],\"type\":\"bar\"}],                        {\"title\":{\"text\":\"Hyperparameter Importances\"},\"xaxis\":{\"title\":{\"text\":\"Hyperparameter Importance\"}},\"yaxis\":{\"title\":{\"text\":\"Hyperparameter\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('575063b3-4301-430f-90ac-f1fd74618bce');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 3. Gr√°fico de importancia de hiperpar√°metros\n",
        "fig2 = optuna.visualization.plot_param_importances(study1)\n",
        "fig2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ac8a20f445d045a3becf1a518d410a7d",
        "deepnote_cell_type": "markdown",
        "id": "EoW32TA9I5wA"
      },
      "source": [
        "## 6. S√≠ntesis de resultados (3 puntos)\n",
        "\n",
        "Finalmente:\n",
        "\n",
        "1. Genere una tabla resumen del MAE en el conjunto de validaci√≥n obtenido en los 5 modelos entrenados desde Baseline hasta XGBoost con Constraints, Optuna y Prunning. [1 punto]\n",
        "2. Compare los resultados de la tabla y responda, ¬øqu√© modelo obtiene el mejor rendimiento? [0.5 puntos]\n",
        "3. Cargue el mejor modelo, prediga sobre el conjunto de **test** y reporte su MAE. [0.5 puntos]\n",
        "4. ¬øExisten diferencias con respecto a las m√©tricas obtenidas en el conjunto de validaci√≥n? ¬øPorqu√© puede ocurrir esto? [1 punto]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "\n",
        "\n",
        "models = {\n",
        "    \"DummyRegressor\": pipeline,\n",
        "    \"XGBoost\": pipeline1,\n",
        "    \"XGBoost_Constrained\": pipeline_xgb_constrained,\n",
        "    \"XGBoost_Optuna\": best_pipeline,\n",
        "    \"XGBoost_Optuna_Pruned\": best_pipeline_pruned}\n",
        "\n",
        "results = []\n",
        "for model_name, model in models.items():\n",
        "    if model is not None:\n",
        "        y_pred_val = model.predict(X_validation)\n",
        "        mae_val = mean_absolute_error(y_validation, y_pred_val)\n",
        "        results.append([model_name, mae_val])\n",
        "    else:\n",
        "        print(f\"Model '{model_name}' not found or not trained. Skipping.\")\n",
        "\n",
        "\n",
        "results_df = pd.DataFrame(results, columns=[\"Model\", \"MAE (Validation)\"])\n",
        "print(results_df)\n",
        "\n",
        "# 2\n",
        "best_model = results_df.loc[results_df[\"MAE (Validation)\"].idxmin()]\n",
        "print(f\"\\nBest model: {best_model['Model']} with MAE {best_model['MAE (Validation)']:.4f}\")\n",
        "\n",
        "# 3\n",
        "best_model_name = best_model[\"Model\"]\n",
        "\n",
        "best_model_pipeline = models[best_model_name]\n",
        "\n",
        "y_pred_test = best_model_pipeline.predict(test_data.drop(columns=['quantity']))\n",
        "mae_test = mean_absolute_error(test_data['quantity'], y_pred_test)\n",
        "print(f\"\\nMAE on test set for {best_model_name}: {mae_test:.4f}\")\n",
        "\n",
        "diff = mae_test - best_model[\"MAE (Validation)\"]\n",
        "\n",
        "print(f\"\\nDifference between validation and test MAE: {diff:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5c4654d12037494fbd385b4dc6bd1059",
        "deepnote_cell_type": "markdown",
        "id": "E_19tgBEI5wA"
      },
      "source": [
        "# Conclusi√≥n\n",
        "Eso ha sido todo para el lab de hoy, recuerden que el laboratorio tiene un plazo de entrega de una semana. Cualquier duda del laboratorio, no duden en contactarnos por mail o U-cursos.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.pinimg.com/originals/55/3d/42/553d42bea9b10e0662a05aa8726fc7f4.gif\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5025de06759f4903a26916c80323bf25",
        "deepnote_cell_type": "markdown",
        "id": "Kq2cFix1I5wA"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown",
        "id": "rAp9UxwiI5wA"
      },
      "source": [
        "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=87110296-876e-426f-b91d-aaf681223468' target=\"_blank\">\n",
        "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
        "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "f63d38450a6b464c9bb6385cf11db4d9",
    "deepnote_persisted_session": {
      "createdAt": "2023-11-09T16:18:30.203Z"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
