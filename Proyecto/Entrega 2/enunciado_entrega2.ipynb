{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "516acf1d6e9d4ddb9a8acdeb6b1cca14",
        "deepnote_app_block_visible": true,
        "deepnote_cell_type": "markdown",
        "id": "-MGJGjPDimJI"
      },
      "source": [
        "![](https://www.dii.uchile.cl/wp-content/uploads/2021/06/Magi%CC%81ster-en-Ciencia-de-Datos.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "befdb70375f04ab79952117eb63723e7",
        "deepnote_app_block_visible": true,
        "deepnote_cell_type": "markdown",
        "id": "SN4W-_BNimJJ"
      },
      "source": [
        "**MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos**\n",
        "\n",
        "### üë®‚Äçüè´üë©‚Äçüè´ Cuerpo Docente:\n",
        "\n",
        "- Profesor: Sebasti√°n Tinoco, Stefano Schiappacasse\n",
        "- Auxiliar: Melanie Pe√±a Torres, Valentina Rojas Osorio\n",
        "- Ayudante: Valentina Zu√±iga, √Ångelo Mu√±oz\n",
        "\n",
        "### üë®‚Äçüíªüë©‚Äçüíª Estudiantes:\n",
        "- Estudiante n¬∞1: Sebasti√°n Acu√±a U.\n",
        "- Estudiante n¬∞2: Mart√≠n Guzm√°n S.\n",
        "\n",
        "_Por favor, lean detalladamente las instrucciones de la tarea antes de empezar a escribir._\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e38a0c399b794718916b5eb2d04e0d91",
        "deepnote_app_block_visible": true,
        "deepnote_cell_type": "markdown",
        "id": "_kBT6Cn2imJJ"
      },
      "source": [
        "## üìö Reglas\n",
        "\n",
        "<center>\n",
        "<img src=\"https://media1.tenor.com/m/0Qtv_cQ4ITsAAAAd/necohaus-grey-name.gif\" width=\"450\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GjshSnpjGcr"
      },
      "source": [
        "El proyecto consta de **dos entregas parciales** y una **entrega final** en donde la primera entrega la idea es poder reflejar lo aprendido durante la primera mitad del curso, que ser√° sobre los contenidos relacionados a *machine learning*, la segunda ser√° sobre los contenidos de la segunda mitad del curso relacionados a *MLOps* y por √∫ltimo la entrega final constar√° de dos partes, donde la primera ser√° relacionada con experimentaci√≥n sobre nuevos datasets que ser√°n disponibilizados durante las √∫ltimas semanas del curso de manera incremental y una segunda parte que ser√° el informe final escrito que deber√° explicar el desarrollo del proyecto completo, como tambien los resultados y an√°lisis de los experimentos realizados sobre los datasets incrementales. La idea es que todo el c√≥digo est√© desarrollado durante las primeras dos entregas y luego en la entrega final s√≥lo se ejecute el c√≥digo sobre nuevos conjuntos de datos.\n",
        "\n",
        "La idea de generar el proyecto por etapas es poder aliviar la carga de trabajo en las √∫ltimas semanas del semestre donde sabemos que est√°n muy cargado con entregas, pruebas y ex√°menes de otros ramos, y as√≠ garantizamos que habiendo la desarrollado las dos primeras entregas parciales, tendr√°n el grueso del proyecto listo para luego experimentar y documentar.\n",
        "\n",
        "---\n",
        "### **Fechas de entrega**\n",
        "- **Entrega parcial 1**: 14 de Mayo\n",
        "- **Entrega parcial 2**: 27 de Junio\n",
        "- **Entrega final**: Por definir\n",
        "\n",
        "---\n",
        "\n",
        "### **Requisitos del proyecto**\n",
        "- **Grupos**: Formar equipos de **2 personas**. No se aceptar√°n trabajos individuales o grupos con m√°s integrantes.\n",
        "- **Consultas**: Cualquier duda fuera del horario de clases debe ser planteada en el foro correspondiente. Los mensajes enviados al equipo docente ser√°n respondidos √∫nicamente por este medio. Por favor, revisen las respuestas anteriores en el foro antes de realizar nuevas consultas.\n",
        "- **Plagio**: La copia o reutilizaci√≥n no autorizada de trabajos de otros grupos est√° **estrictamente prohibida**. El incumplimiento de esta norma implicar√° la anulaci√≥n inmediata del proyecto y una posible sanci√≥n acad√©mica.\n",
        "- **Material permitido**: Pueden usar cualquier material del curso, ya sea notas, lecturas, c√≥digos, o referencias proporcionadas por los docentes, que consideren √∫til para el desarrollo del proyecto.\n",
        "\n",
        "---\n",
        "\n",
        "### **Entregables y etapas**\n",
        "\n",
        "#### **1. Entrega Parcial 1**  \n",
        "- Dispondr√°n de los archivos de datos **productos.parquet**, **clientes.parquet** y **transacciones.parquet** para el modelamiento inicial.  \n",
        "- Utilizar√°n estos archivos para desarrollar lo solicitado para la entrega 1.\n",
        "- En esta etapa, se espera que apliquen todos los conocimientos aprendidos durante la primera parte del curso relacionados con *machine learning*.\n",
        "- **Informe**: No se exige un avance del informe en esta etapa, s√≥lo un notebook con su desarrollo actual, pero se **recomienda comenzar** a redactar el informe final en paralelo para disminuir la carga acad√©mica en las etapas posteriores.  \n",
        "\n",
        "#### **2. Entrega Parcial 2**  \n",
        "- En esta entrega, deber√°n aplicar los conocimientos aprendidos durante la segunda mitad del curso sobre *MLOps*  \n",
        "- Se espera que implementen estos conocimientos para desplegar su modelo elegido en la primera entrega y crear *pipelines* automatizados que simulen un entorno productivo.\n",
        "- **Informe**: Similar a la primera etapa, no se exige un avance del informe, pero se **recomienda avanzar con su redacci√≥n** para evitar una acumulaci√≥n de trabajo en la etapa final.  \n",
        "\n",
        "#### **3. Entrega Final**  \n",
        "- En la entrega final, deber√°n realizar dos etapas:\n",
        "\t- La primera etapa es sobre experimentaci√≥n utilizando datasets incrementales que se ir√°n disponibilizando de manera parcial, para que vayan generando predicciones con su modelo ya desplegado. El objetivo de esta etapa es poder testear su soluci√≥n *end-to-end* y que vayan analizando los resultados obtenidos a medida que se van agregando m√°s datos.\n",
        "\t- La segunda etapa consiste en redactar un informe final que deber√° explicar el desarrollo completo de tu proyecto y un an√°lisis profundo de sus resultados de experimentaci√≥n. Este informe debera incluir a lo menos las siguientes secciones:\n",
        "\t\t- An√°lisis exploratorio de datos  \n",
        "\t\t- Metodolog√≠a aplicada  \n",
        "\t\t- Selecci√≥n y entrenamiento de modelos  \n",
        "\t\t- Evaluaci√≥n de resultados  \n",
        "\t\t- Optimizaci√≥n de modelos\n",
        "\t\t- Interpretabilidad\n",
        "\t\t- Re-entrenamiento\n",
        "\t\t- Tracking con MLFlow\n",
        "\t\t- Creaci√≥n de la aplicaci√≥n web con Gradio y FastAPI\n",
        "\n",
        "Es **altamente recomendable** ir redactando el informe en paralelo al desarrollo de los modelos para garantizar que toda la informaci√≥n relevante quede documentada adecuadamente.  \n",
        "\n",
        "### Nota Final\n",
        "\n",
        "La calificaci√≥n final de su proyecto se calcular√° utilizando la siguiente ponderaci√≥n:\n",
        "\n",
        "$$Nota Final = 0.30 * EntregaParcial1 + 0.40 * EntregaParcial2 + 0.30 * EntregaFinal$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Instrucciones importantes**\n",
        "\n",
        "1. **Formato del informe**:  \n",
        "   - El informe debe estar integrado dentro de un **Jupyter Notebook**. No es necesario subirlo a una plataforma externa, pero debe cumplir con los siguientes requisitos:  \n",
        "     - Estructura clara y ordenada.  \n",
        "     - C√≥digo acompa√±ado de explicaciones detalladas.  \n",
        "     - Resultados presentados de forma visual y anal√≠tica.  \n",
        "\n",
        "2. **Descuento por informes deficientes**:  \n",
        "   - Cualquier secci√≥n del informe que no tenga una explicaci√≥n adecuada o no respete el formato ser√° penalizada con un descuento en la nota. Esto incluye c√≥digo sin comentarios o an√°lisis que no sean coherentes con los resultados presentados.\n",
        "   - **Comentarios sin formatear de ChatGPT o herramientas similares ser√°n penalizados (e.g: \"Inserta tu modelo ac√°\", etc.)**\n",
        "\n",
        "<center>\n",
        "<img src=\"https://media1.giphy.com/media/ZkIUY6LZJMwrSMQMWu/giphy.gif?cid=6c09b952msc755426bpsadn4sysofcpqczczi7x1pghd41sw&ep=v1_internal_gif_by_id&rid=giphy.gif&ct=v\" width=\"400\" height=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ad10b0086d0241a3b3063e36555d6b9f",
        "deepnote_app_block_visible": true,
        "deepnote_cell_type": "markdown",
        "id": "LDShUyH4imJK"
      },
      "source": [
        "## Notas adicionales\n",
        "- No necesitan tener un rendimiento cercano al 100% para tener una resoluci√≥n exitosa en el proyecto.\n",
        "- Utilizar paralelizaci√≥n para acelerar b√∫squedas. Esto podr√≠a ser una buena soluci√≥n para el caso de que la b√∫squeda de hiperpar√°metros sea muy lenta. En caso de tener problemas de RAM, reducir la cantidad de `jobs` a algo que su computador/interprete web pueda procesar.\n",
        "- Generar grillas de b√∫squedas razonables. Entre m√°s grande es la grilla, m√°s lento el proceso de b√∫squeda. Utilice grillas de tama√±os adecuados para que la b√∫squeda converga en tiempos razonables y no se demore 3.5 eternidades en terminar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "73dd6ad576224431b010e7650d06156e",
        "deepnote_app_block_visible": true,
        "deepnote_cell_type": "markdown",
        "id": "mMwIvE7AimJK"
      },
      "source": [
        "# üì¨ Entrega Parcial 2 (40% del Proyecto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbrEry-nnsfs"
      },
      "source": [
        "### üì™ Fecha de Entrega: 27 de Junio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S9iswjPnxmu"
      },
      "source": [
        "## üìñ Enunciado\n",
        "\n",
        "Gracias al impecable trabajo que presento su dupla, la primera etapa del proyecto fue un √©xito rotundo! Pero no hay tiempo para descansar...\n",
        "\n",
        "<center>\n",
        "<img src=\"https://media.tenor.com/ibeRrcBcNFAAAAAM/theres-no-time-to-waste-barack-obama.gif\" width=\"350\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a2fbf90275b84abc9efb217f8722a11d",
        "deepnote_app_block_visible": true,
        "deepnote_cell_type": "markdown",
        "id": "MVh_MqB8imJK"
      },
      "source": [
        "Tras haber entrenado su primer modelo predictivo, el equipo **Deep Drinkers ü§ñ** ha demostrado que tiene lo necesario para revolucionar la manera en que se toman decisiones comerciales dentro de **SodAI Drinks ü•§**. El comit√© de innovaci√≥n ha quedado tan impresionado que les ha encomendado una nueva y m√°s desafiante misi√≥n.\n",
        "\n",
        "En esta segunda fase del proyecto, deber√°n fortalecer su soluci√≥n y prepararla para el mundo real (tambi√©n conocido como producci√≥n). La idea es que tomen el modelo elegido en la primera entrega y lo utilicen para implementar toda esta segunda entrega que tendr√° como objetivo √∫ltimo, disponibilizar su modelo en producci√≥n para que pueda ser consumido a trav√©s de la web. Entonces, deben aplicar lo aprendido en las clases relacionadas a `MLOps` (despliegue, `Docker`, *pipeline productivo*, monitoreo), en conjunto con los conocimientos adquiridos en la primera parte del curso (`Mlflow`, `SHAP`, `sklearn`, entre otros.) e implementarlo sobre su modelo.\n",
        "\n",
        "Esta nueva misi√≥n que se les ha encargado incluye que realicen, a lo menos, las siguientes partes.\n",
        "\n",
        "‚ö†Ô∏è Advertencia importante: **SodAI Drinks** ü•§ no quiere tener que despedir a sus empleados estrella si su modelo empieza a hacer predicciones incoherentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìåPipelines Productivos [3.0 puntos]\n",
        "<center>\n",
        "<img src=\"https://media.tenor.com/ACThgg-lfvEAAAAM/goofy-working-hard.gif\" width=\"400\" height=\"250\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En esta etapa, el equipo debe dise√±ar e implementar una canalizaci√≥n automatizada utilizando `Apache Airflow` para orquestar de forma eficiente todo el flujo de trabajo de su sistema predictivo, integrando herramientas como `MLflow`, `Optuna` y `SHAP`. Esta canalizaci√≥n debe contemplar el ciclo completo de modelado: desde la extracci√≥n de datos hasta la generaci√≥n de predicciones y el seguimiento de resultados, de manera robusta y flexible.\n",
        "\n",
        "Aunque en esta entrega se trabajar√° √∫nicamente con los datos iniciales proporcionados, es **fundamental** que el *pipeline* sea dise√±ado con visi√≥n de producci√≥n, tal como se esperar√≠a en un entorno empresarial real. Por tanto, el *pipeline* debe estar preparado para detectar *drift* en los datos y, de ser necesario, reentrenar el modelo de forma automatizada ante la llegada de nuevos datos.\n",
        "\n",
        "**Nota importante: T√≥mense un tiempo para leer detenidamente esto para entender lo que esto significa. Para entrega final del proyecto se les pedir√° que su pipeline reciba un nuevo conjunto de datos (por ejemplo, ustedes cuentan con datos (t) y reciben una nueva semana de datos (t+1)) y genere la predicci√≥n para la semana siguiente (t+2). <u>Repetiremos este ejercicio varias veces</u>. Tome esto en consideraci√≥n para el desarrollo de esta secci√≥n.**\n",
        "\n",
        "**BONUS**: Si deciden utilizar `MLflow`, se les otorgar√° entre 0 y 0.5 puntos de puntaje adicional dependiendo su uso y en qu√© partes lo utilizan.\n",
        "\n",
        "Su *pipeline* debe considerar los siguientes puntos:\n",
        "\n",
        "- **Extracci√≥n de datos**: Configurar `Airflow` para obtener autom√°ticamente los datos y prepararlos como se realiz√≥ en la primera entrega. \n",
        "  - Asuman que los nuevos datos cuentan con la misma estructura de `transacciones.parquet`.\n",
        "  - Para esta parte, pueden asumir que los datos aparecen *m√°gicamente* en su directorio de trabajo.\n",
        "  - Pueden realizar supuestos adicionales, pero estos deben quedar evidenciados de forma clara y f√°cil de distinguir para su posterior revisi√≥n.\n",
        "- **Limpieza y transformaci√≥n**: Estandarizar y preparar los datos para su uso en el modelo. Esta etapa debe ser modular y replicable para nuevos datos.\n",
        "- **Detecci√≥n de drift en los datos [BONUS: 0.3 puntos]**: Desarrollar un sistema o modelo que sea capaz de distinguir cuando exista *drift* en los datos.\n",
        "- **Reentrenamiento del modelo**: Implementar una rutina de reentrenamiento y trackeo de resultados. Adem√°s, esta etaba debe considerar:\n",
        "    - Si deciden implementar la detecci√≥n de *drift*, este paso s√≥lo debe ser ejecutado en caso de que exista drift en los datos. En caso contrario, deben desarrollar c√≥digo para que el reentrenamiento sea ejecutado de forma peri√≥dica.\n",
        "    - Optimizaci√≥n de hiperpar√°metros.\n",
        "    - Entrenar modelo con los hiperpar√°metros seleccionados usando la data nueva.\n",
        "    - Trackeo de sus resultados, registrando de manera organizada la siguiente informaci√≥n (se recomienda much√≠simo usar `MLflow`):\n",
        "      - M√©tricas de desempe√±o\n",
        "      - Hiperpar√°metros del modelo\n",
        "      - Gr√°ficos de interpretabilidad.\n",
        "    - Exportar el modelo entrenado para su posterior uso (se recomienda usar `MLflow` para esto). \n",
        "\n",
        "    Esta l√≥gica puede quedar implementada como tareas condicionales, *placeholders* o m√≥dulos configurables, que se activar√°n m√°s adelante.\n",
        "  \n",
        "- **Generaci√≥n de predicciones**  \n",
        "  - Utilizar el mejor modelo disponible para predecir, para cada par cliente-producto, si el cliente comprar√° ese producto la pr√≥xima semana.  \n",
        "  - **Salida**: el *pipeline* debe generar un **archivo `.csv`** que contenga **solo** las combinaciones *cliente-producto* con predicci√≥n positiva (es decir, los productos que se espera que el cliente compre). NO deben incluirse los pares con predicci√≥n negativa.\n",
        "\n",
        "\n",
        "**IMPORTANTE**: Como \"pr√≥xima semana\" deben usar la **semana siguiente a la semana m√°s reciente presente en los datos**. Por ejemplo, si sus datos contienen hasta la √∫ltima semana del 2024, se le pide generar predicciones para la primera semana del 2025.\n",
        "\n",
        "Para esta secci√≥n se esperan los siguientes entregables:\n",
        "\n",
        "* **C√≥digo del DAG de Airflow [2.0 puntos]**: \n",
        "  * Archivos Python con la definici√≥n completa del `DAG` y todos sus operadores.\n",
        "  * Scripts auxiliares para las diferentes tareas, si lo estima necesario (transformaci√≥n, entrenamiento, evaluaci√≥n, interpretabilidad, entre otros).\n",
        "  * Configuraci√≥n del entorno `Airflow` (requisitos, conexiones, variables, entre otros).\n",
        "  * Todo el c√≥digo debe ser reproducible.\n",
        "\n",
        "* **Video de ejecuci√≥n del pipeline [0.5 puntos]**\n",
        "  * Deben adjuntar un video mostrando la ejecuci√≥n del pipeline de `Airflow` siendo ejecutado de principio a fin.\n",
        "  * El pipeline debe ser ejecutado considerando un nuevo conjunto de datos de entrada (pueden generarlo de forma aleatoria, extraer una semana cualquiera del dataset, etc).\n",
        "  * La ejecuci√≥n del pipeline DEBE mostrar como los datos pasan por la etapa de reentrenamiento. Pueden desarrollar flujos condicionales  para esto.\n",
        "  * No es necesario que suban este video al repositorio ni a u-cursos. En su lugar, pueden subirlo a Youtube o alguna otra plataforma de su preferencia y enviarnos el link de acceso junto a su entrega.\n",
        "\n",
        "* **Documentaci√≥n detallada en markdown [0.5 puntos]**:\n",
        "  * Una descripci√≥n clara del `DAG`, explicando la funcionalidad de cada tarea y c√≥mo se relacionan entre s√≠.\n",
        "  * Diagrama de flujo del *pipeline* completo.\n",
        "  * Una representaci√≥n visual del `DAG` en la interfaz de `Airflow`.\n",
        "  * Explicaci√≥n de c√≥mo se dise√±√≥ la l√≥gica para integrar futuros datos, detectar *drift* y reentrenar el modelo.\n",
        "  * Todos estos puntos deben estar contenidos en un archivo markdown junto a la carpeta de esta secci√≥n.\n",
        "\n",
        "* **Los puntos anteriores deben estar contenidos de forma ordenada en una carpeta llamada `airflow`**\n",
        "\n",
        "üëÄ **Hint**: *Se recomienda revisar la siguiente documentaci√≥n de [MLFlow](https://mlflow.org/docs/latest/tracking/#quickstart) y [Airflow](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/fundamentals.html) para el desarrollo de esta secci√≥n.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMYZOfWptMrC"
      },
      "source": [
        "## üìå Desarrollo de Aplicaci√≥n Web [2.0 puntos]\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*eGHIz7MqdP_tUGGVNnDfWA.jpeg\" width=\"500\" height=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b717eb9e1c3b4b7ca0648289d90be804",
        "deepnote_app_block_visible": true,
        "deepnote_cell_type": "markdown",
        "id": "F3Bz6_4simJK"
      },
      "source": [
        "Una vez implementado el *pipeline* productivo, el siguiente paso es desarrollar una interfaz que permita a los usuarios finales de **SodAI Drinks ü•§** interactuar con el modelo desplegado. Esta aplicaci√≥n web debe consumir las predicciones generadas por el *pipeline* desarrollado en la secci√≥n anterior, estableciendo as√≠ un flujo completo desde el procesamiento de datos hasta la visualizaci√≥n de resultados.\n",
        "\n",
        "En esta etapa, deber√°n construir una aplicaci√≥n web que integre un *frontend* desarrollado en `Gradio` y un *backend* con `FastAPI`, asegurando la comunicaci√≥n efectiva entre ambos componentes y con el *pipeline* previamente implementado. Para garantizar la portabilidad y facilidad del despliegue en diferentes entornos, tanto el *backend* como el *frontend* deben ser dockerizados.\n",
        "\n",
        "Para esta secci√≥n se esperan los siguientes entregables:\n",
        "\n",
        "* **Backend con FastAPI [0.9 puntos]**: \n",
        "  * Crear rutas (*endpoints*) para recibir solicitudes y devolver predicciones.\n",
        "  * Cargar el modelo entrenado. En caso de haber utilizado `MLFlow`, se les recomienda usarlo para cargar el modelo directamente.\n",
        "  * Procesar los datos que recibe la aplicaci√≥n.\n",
        "  * Dockerfile para levantar contenedor.\n",
        "  * Todo su desarrollo debe estar contenido en una carpeta `backend`\n",
        "\n",
        "* **Frontend con Gradio [0.9 puntos]**:\n",
        "  * Crear una interfaz amigable donde los usuarios puedan introducir los datos.\n",
        "  * Mostrar los resultados de las predicciones de forma clara.\n",
        "  * Incluir explicaciones sobre c√≥mo usar la aplicaci√≥n (esto debe estar en la interfaz).\n",
        "  * Dockerfile para levantar contenedor.\n",
        "  * Todo su desarrollo debe estar contenido en una carpeta `frontend`\n",
        "\n",
        "* **Un archivo `docker-compose.yml` que permita ejecutar ambos contenedores juntos. [0.2 puntos]**\n",
        "\n",
        "* **Los puntos anteriores deben estar contenidos de forma ordenada en una carpeta llamada `app`.**\n",
        "\n",
        "* **Importante: Se revisar√° esta secci√≥n levantando la aplicaci√≥n que entreguen mediante Docker. Naturalmente, si la aplicaci√≥n tiene problemas en levantar se les descontar√° puntaje.**\n",
        "\n",
        "üëÄ **Hint**: *Se recomienda revisar el cheatsheet de [Dockerlabs](https://dockerlabs.collabnix.com/docker/cheatsheet/) para optimizar la creaci√≥n de las im√°genes y contenedores.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ36o9Sc5lp6"
      },
      "source": [
        "## üìåConclusiones  [1.0 punto]\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i0.wp.com/vanessabrooks2020.com/wp-content/uploads/2017/12/the-end-question-mark.gif?fit=498%2C256&ssl=1\" width=\"500\" height=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En esta secci√≥n deben reflexionar sobre los aprendizajes, desaf√≠os y oportunidades del enfoque `MLOps` aplicado. Sus reflexiones deben ser escritas en un archivo `conclusiones.md`\n",
        "\n",
        "Algunas ideas que pueden abordar:\n",
        "- ¬øC√≥mo mejor√≥ el desarrollo del proyecto al utilizar herramientas de *tracking* y despliegue?\n",
        "- ¬øQu√© aspectos del despliegue con `Gradio/FastAPI` fueron m√°s desafiantes o interesantes?\n",
        "- ¬øC√≥mo aporta `Airflow` a la robustez y escalabilidad del pipeline?\n",
        "- ¬øQu√© se podr√≠a mejorar en una versi√≥n futura del flujo? ¬øQu√© partes automatizar√≠an m√°s, qu√© monitorear√≠an o qu√© m√©tricas agregar√≠an?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéÅ Bonus [1.0 puntos]\n",
        "\n",
        "De forma adicional a todo el desarrollo anterior, pueden optar a obtener puntaje adicional desarrollando los siguientes puntos:\n",
        "\n",
        "* **Sistema de Recomendaci√≥n [0.5 puntos]**: \n",
        "    * Debe generar 5 recomendaciones de productos para cualquier cliente.\n",
        "    * Debe entregar su soluci√≥n separando backend y frontend, ambos dockerizados y comunic√°ndose. \n",
        "    * Todo su desarrollo debe estar contenido en la carpeta `recsys`\n",
        "\n",
        "* **Chatbot Conversacional [0.5 puntos]**:\n",
        "    * Debe ser capaz de responder dudas sobre los datos existentes, por ejemplo:\n",
        "        * ¬øCu√°ntos clientes √∫nicos hay en el dataset?\n",
        "        * ¬øCu√°ntas transacciones ha realizado el cliente X?\n",
        "        * ¬øCu√°ntos productos √∫nicos se encuentran en los datos?\n",
        "    * Debe entregar su soluci√≥n separando backend y frontend, ambos dockerizados y comunic√°ndose.\n",
        "    * Todo su desarrollo debe estar contenido en la carpeta `llm`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÅ Estructura de entrega\n",
        "\n",
        "Se espera que su entrega cuente con la siguiente estructura de carpetas:\n",
        "\n",
        "```bash\n",
        "entrega_2/\n",
        "‚îÇ\n",
        "‚îú‚îÄ‚îÄ airflow/ # Carpeta con todos los scripts de la secci√≥n Pipelines Productivos\n",
        "‚îÇ\n",
        "‚îú‚îÄ‚îÄ app/\n",
        "‚îÇ ‚îú‚îÄ‚îÄ backend/ # Carpeta con scripts del backend\n",
        "‚îÇ ‚îú‚îÄ‚îÄ frontend/ # Carpeta con scripts del frontend\n",
        "‚îÇ ‚îî‚îÄ‚îÄ docker-compose.yml\n",
        "‚îÇ\n",
        "‚îú‚îÄ‚îÄ bonus/\n",
        "‚îÇ ‚îú‚îÄ‚îÄ recsys/ # Carpeta con scripts del bonus de Sistemas de Recomendaci√≥n\n",
        "‚îÇ ‚îî‚îÄ‚îÄ llm/ # Carpeta con scripts del bonus de Chat Conversacional\n",
        "‚îÇ\n",
        "‚îî‚îÄ‚îÄ conclusiones.md # Conclusiones del trabajo realizado\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**RECOMENDACI√ìN** \n",
        "\n",
        "En caso de que a√∫n no lo hagan, les recomendamos fuertemente que migren su c√≥digo a un repositorio (separado del repo del curso). Este paso es necesario ya que para la entrega final requerir√°n un repositorio independiente y bien estructurado.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.pinimg.com/originals/f4/75/e1/f475e17d2d74c96d45ac92b14de16da5.gif\" width=\"200\" height=\"200\">\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM6rrqJ85tmz"
      },
      "source": [
        "Mucho √©xito!\n",
        "\n",
        "<center>\n",
        "<img src=\"https://gifdb.com/images/high/may-the-force-be-with-you-anakin-skywalker-n2g5o0pm4h6iylsx.gif\" width=\"400\" height=\"300\">\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "deepnote_app_layout": "powerful-article",
    "deepnote_app_reactivity_enabled": true,
    "deepnote_notebook_id": "2939a76dd7c14f7f948714e40aa8bd20",
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
